{"meta":{"title":"LsWisdom的博客","subtitle":"谋事在人成事在天","description":"谋事在人成事在天","author":"ls","url":"https://lswisdom.github.io","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-11-14T15:08:05.826Z","updated":"2021-08-15T12:28:37.832Z","comments":false,"path":"/404.html","permalink":"https://lswisdom.github.io/404.html","excerpt":"","text":""},{"title":"分类","date":"2021-11-14T15:08:05.827Z","updated":"2021-08-15T12:28:37.850Z","comments":false,"path":"categories/index.html","permalink":"https://lswisdom.github.io/categories/index.html","excerpt":"","text":""},{"title":"友情链接1","date":"2022-10-29T09:16:02.652Z","updated":"2022-10-29T09:16:02.652Z","comments":true,"path":"links/index.html","permalink":"https://lswisdom.github.io/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2021-11-14T15:08:05.830Z","updated":"2021-08-15T12:28:37.853Z","comments":false,"path":"repository/index.html","permalink":"https://lswisdom.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-11-14T15:08:05.831Z","updated":"2021-08-15T12:28:37.854Z","comments":false,"path":"tags/index.html","permalink":"https://lswisdom.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Docker安装","slug":"服务器运维/Docker安装/Docker安装","date":"2022-10-29T07:01:26.000Z","updated":"2022-10-29T08:00:12.447Z","comments":true,"path":"posts/708881443/","link":"","permalink":"https://lswisdom.github.io/posts/708881443/","excerpt":"","text":"Docker安装1.配置docker安装12345678910# step 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;centos&#x2F;docker-ce.repo# Step 3: 更新并安装 Docker-CEsudo yum makecache fastsudo yum -y install docker-ce# Step 4: 开启Docker服务sudo service docker start 2.配置镜像加速器12345678sudo mkdir -p &#x2F;etc&#x2F;dockersudo tee &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;-&#39;EOF&#39;&#123; &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;xxxx.mirror.aliyuncs.com&quot;] # 配置自己的阿里云进行站&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker &lt;%- search_form(options) %&gt; Post not found: hashmap https://lswisdom.com/posts/1082886779/ &lt;%- link_to(‘https://lswisdom.com/posts/1082886779/&#39;, [text], [options]) %&gt;","categories":[{"name":"k8s安装","slug":"k8s安装","permalink":"https://lswisdom.github.io/categories/k8s%E5%AE%89%E8%A3%85/"}],"tags":[{"name":"运维环境部署","slug":"运维环境部署","permalink":"https://lswisdom.github.io/tags/%E8%BF%90%E7%BB%B4%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"}]},{"title":"VUE基本使用和的组件介绍","slug":"前端框架/vue的基本使用","date":"2021-08-20T13:37:58.000Z","updated":"2021-08-21T08:26:16.518Z","comments":true,"path":"posts/1082886779/","link":"","permalink":"https://lswisdom.github.io/posts/1082886779/","excerpt":"","text":"Vue的简单使用一简介Vue是一款渐进式的JavaScript框架,作者尤雨溪(Evan You)Vue.js 的核心是一个允许采用简洁的模板语法来声明式地将数据渲染进 DOM 的系统 特点 更加轻量20kb min +gzip 渐进式框架 响应式的更新机制 学习成本低 官网vue 官网地址：https://cn.vuejs.org/v2/guide/#Vue-js-%E6%98%AF%E4%BB%80%E4%B9%88安装分为两个版本,开发版本和生成版本,如下图所示： HelloWorld开发工具vscode新建一个html文件,输入html:5 快速生成一个模板 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;!--双括号用来显示变量 双括号还支持表达式如：&#123;&#123;message+message&#125;&#125; ,但是不支持定义变量 --&gt;&lt;div id=&quot;app&quot;&gt;&#123;&#123; message &#125;&#125; &#123;&#123; message + message &#125;&#125; &lt;!-- 动态解析v-bind 可以简写为:id v-bind 用来绑定一个动态的值 --&gt; &lt;div :id=&quot;message&quot;&gt;&lt;/div&gt; &lt;ul&gt; &lt;li v-for=&quot;item in list&quot;&gt; &lt;!-- v-if 和 v-else 和js中的if else作用是相似的 v-show 是否显示按钮--&gt; &lt;!-- v-for对应这js中的循环操作 --&gt; &lt;span v-if=&quot;!item.del_flag&quot;&gt;&#123;&#123; item.title &#125;&#125;&lt;/span&gt; &lt;span v-else style=&quot;text-decoration: line-through;&quot;&gt;&#123;&#123; item.title &#125;&#125;&lt;/span&gt; &lt;button v-show=&quot;!item.del_flag&quot;&gt;删除&lt;/button&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;!-- 引入CDN 文件--&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.js&quot;&gt;&lt;/script&gt;&lt;script&gt; // 直接下载并用 &lt;script&gt; 标签引入，Vue 会被注册为一个全局变量 var vm = new Vue(&#123; el: &#x27;#app&#x27;, // el元素用来指定一个DOM节点。div标签使用是ID,所以这里使用警号 data: &#123; // data表示数据 message: &#x27;hello world&#x27;, list: [&#123; title: &#x27;标题1&#x27;, del_flag: false &#125;, &#123; title: &#x27;标题2&#x27;, del_flag: true &#125;] &#125; &#125;)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 现在数据和 DOM 已经被建立了关联，所有东西都是响应式的。可以打开你的浏览器的 JavaScript 控制台 并修改 vm.message 的值，你将看到上例相应地更新。v-bind attribute 被称为指令。指令带有前缀 v-，以表示它们是 Vue 提供的特殊 attribute。可能你已经猜到了，它们会在渲染的 DOM 上应用特殊的响应式行为。 基本语法v-if控制切换一个元素 v-for指令可以绑定数组的数据来渲染一个项目列表 v-on指令添加一个事件监听器，通过它调用在 Vue 实例中定义的方法,例如： 1234567891011121314151617181920212223242526&lt;body&gt;&lt;div id=&quot;app2&quot;&gt; &lt;div&gt;&#123;&#123;message&#125;&#125;&lt;/div&gt; &lt;!-- v-on 绑定一个点击时间 --&gt; &lt;button v-on:click=&quot;findUser&quot;&gt; 反转message信息 &lt;/button&gt;&lt;/div&gt;&lt;!-- 引入CDN 文件--&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.js&quot;&gt;&lt;/script&gt;&lt;script&gt; var vm = new Vue(&#123; el: &#x27;#app2&#x27;, // el用来定义元素 data:&#123; // data用来定义数据 message: &#x27;Hello Vue.js!&#x27; &#125;, methods: &#123; //methods用来定义方法 findUser: function ()&#123; this.message = this.message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;) &#125; &#125; &#125;)&lt;/script&gt;&lt;/body&gt; 生命周期钩子函数含义每个 Vue 实例在被创建时都要经过一系列的初始化过程——例如，需要设置数据监听、编译模板、将实例挂载到 DOM 并在数据变化时更新 DOM 等。同时在这个过程中也会运行一些叫做生命周期钩子的函数，这给了用户在不同阶段添加自己的代码的机会。 生命周期图示下图是官网提供的声明周期图示 代码demo12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;body&gt;&lt;!-- 引入CDN 文件--&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.js&quot;&gt;&lt;/script&gt;&lt;!--Vue的声明周期演示--&gt;&lt;div id=&quot;app&quot;&gt; &#123;&#123;msg&#125;&#125;&#125;&lt;/div&gt;&lt;script&gt; // Vue对象创建的时候，需要传入一个对象 var vm = new Vue(&#123; el: &#x27;#app&#x27;, data: &#123; msg: &#x27;hi vue&#x27; &#125;, // 在实例初始化之后,数据观测(data observer) 和 event/watcher 事件配置之前调用 beforeCreate: function () &#123; console.log(&#x27;beforeCreate&#x27;); &#125;, // 在实例创建过程后立即被调用 // 在这一步,实例已完成以下配置，数据观测(data observer) 属性和方法的运算, watch/event 事件回调 created: function () &#123; console.log(&#x27;created&#x27;) &#125;, // 在挂在开始之前被调用,相关的渲染函数首次被调用 beforeMount: function () &#123; console.log(&#x27;beforeMount&#x27;) &#125;, // el 被新建的vm.$el 替换 挂在成功 mounted: function () &#123; console.log(&#x27;mounted&#x27;) &#125;, // 数据更新前调用 beforeUpdate:function () &#123; console.log(&#x27;beforeUpdate&#x27;) &#125;, // 组件Dom已经更新 updated :function () &#123; console.log(&#x27;updated&#x27;) &#125; &#125;); // 3s之后把msg的属性值从hi vue 替换为change setTimeout(function () &#123; vm.msg = &quot;chang..........&quot;; &#125;,3000)&lt;/script&gt;&lt;/body&gt; 演示效果如下，可以看到控制台函数的一个打印顺序 VUE组件組件是什么允许我们使用小型、独立和通常可复用的组件构建大型应用,在 Vue 里，一个组件本质上是一个拥有预定义选项的一个 Vue 实例例如，你可能会有页头、侧边栏、内容区等组件，每个组件又包含了其它的像导航链接、博文之类的组件。为了能在模板中使用，这些组件必须先注册以便 Vue 能够识别。这里有两种组件的注册类型：全局注册和局部注册。至此，我们的组件都只是通过Vue.component全局注册的：全局注册的组件可以用在其被注册之后的任何 (通过 new Vue) 新创建的 Vue 根实例，也包括其组件树中的所有子组件的模板中。组织结构图如下： 怎么用Vue.component :组件固定格式：包含两个参数，参数一：组件的名称,该值必须唯一。参数二：组建的对象。里面可以包含方法、数据template : 组件里面需要有一个模板，这个参数表示组件的模板。需要复用的html模板props : 可选操作,Prop 是你可以在组件上注册的一些自定义 attribute。当一个值传递给一个 prop attribute 的时候，它就变成了那个组件实例的一个 property 一个组件默认可以拥有任意数量的 prop，任何值都可以传递给任何 prop 组件之间也可以互相包含,模板如下： 123Vue.component(&#x27;my-component-name&#x27;, &#123; // ... options ...&#125;) 练习基本Demo123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt; &lt;meta charset=&quot;UTF-8&quot;&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt; &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;ol&gt;&lt;/ol&gt;&lt;!-- 引入CDN 文件--&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.js&quot;&gt;&lt;/script&gt;&lt;div id=&quot;app&quot;&gt; &lt;div&gt;&#123;&#123; message&#125;&#125;&lt;/div&gt;&lt;!-- &lt;ul&gt;--&gt;&lt;!-- &lt;todo-item v-for=&quot; item in list &quot; :title=&quot;item.title&quot; :del_flag=&quot;item.del_flag&quot;&gt;&lt;/todo-item&gt;--&gt;&lt;!-- &lt;/ul&gt;--&gt; &lt;todo-list&gt;&lt;/todo-list&gt; &lt;todo-list&gt;&lt;/todo-list&gt; &lt;todo-list&gt;&lt;/todo-list&gt;&lt;/div&gt;&lt;script&gt; // todo-itme:表示组件的名称 // &#123;&#125;表示组件的对象 // template 表示组件的模板 Vue.component(&#x27;todo-item&#x27;,&#123; // 为了进行复用，模板中定义两个变量。定义一个title 和一个del_flag,并通过props 进行属性传播 props:&#123; // 定义属性名称：属性类型 title: String, // 定义一个del_flag属性，并设置为Boolean 默认值为false del_flag:&#123; type: Boolean, default: false &#125; &#125;, // 注意：组件模板必须包含一个根元素，否则控制台会报错 template:&#x27;&lt;div&gt;&lt;/iv&gt; &lt;span v-if=&quot;!del_flag&quot;&gt;&#123;&#123;title &#125;&#125;&lt;/span&gt;\\n&#x27; + &#x27; &lt;span v-else style=&quot;text-decoration: line-through;&quot;&gt;&#123;&#123;title &#125;&#125;&lt;/span&gt;\\n&#x27; + &#x27; &lt;button v-show=&quot;!del_flag&quot;&gt;删除&lt;/button&gt;&lt;/div&gt;&#x27; , // 定义data类型返回数据的时候，最好使用一个自定义函数进行返回 data: function () &#123; return &#123;&#125; &#125;, methods: &#123; &#125; &#125;) Vue.component(&#x27;todo-list&#x27;,&#123; // 组件包含组件的 template: &#x27; &lt;ul&gt;\\n&#x27; + &#x27; &lt;todo-item v-for=&quot; item in list &quot; :title=&quot;item.title&quot; :del_flag=&quot;item.del_flag&quot;&gt;&lt;/todo-item&gt;\\n&#x27; + &#x27; &lt;/ul&gt;&#x27;, data: function () &#123; return &#123; list: [&#123; title: &#x27;标题1&#x27;, del_flag: false &#125;,&#123; title: &#x27;标题2&#x27;, del_flag: true &#125;] &#125; &#125; &#125;) // 创建一个Vue的实例 let vm = new Vue(&#123; // el 元素标签，绑定一个div el: &#x27;#app&#x27;, data: &#123; message: &#x27;这是一个Vue的组件demo&#x27;, list: [&#123; title: &#x27;标题1&#x27;, del_flag: false &#125;,&#123; title: &#x27;标题2&#x27;, del_flag: true &#125;] &#125; &#125;)&lt;/script&gt;&lt;/body&gt; 练习2：多元素属性12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;body&gt;&lt;!-- 引入CDN 文件--&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.js&quot;&gt;&lt;/script&gt;&lt;div id=&quot;app&quot;&gt; &lt;div&gt;&#123;&#123; message&#125;&#125;&lt;/div&gt; &lt;ul&gt; &lt;!-- 这里传入了多个元素,假设有10个元素的话，一个个的书写就很麻烦，组件对象中编写的时候，也需要定义属性的名称和类别 --&gt; &lt;todo-item v-for=&quot; item in list &quot; v-bind:title=&quot;item.title&quot; v-bind:del_flag=&quot;item.del_flag&quot; v-bind:content=&quot;item.content&quot; &gt;&lt;/todo-item&gt; &lt;/ul&gt;&lt;/div&gt;&lt;script&gt; // todo-itme:表示组件的名称 // &#123;&#125;表示组件的对象 // template 表示组件的模板 Vue.component(&#x27;todo-item&#x27;,&#123; // 为了进行复用,如果变量有很多的时候,可知直接传入一个变量,取名叫做item // props:[&#x27;item&#x27;], props:&#123; title: String, del_flag: Boolean, content: String &#125;, // 注意：组件模板必须包含一个根元素，否则控制台会报错 template:&#x27;&lt;div&gt;&lt;/iv&gt; &lt;span v-if=&quot;!del_flag&quot;&gt;&#123;&#123;title &#125;&#125;&lt;/span&gt;\\n&#x27; + &#x27; &lt;span v-else style=&quot;text-decoration: line-through;&quot;&gt;&#123;&#123;title &#125;&#125;&lt;/span&gt;\\n&#x27; + &#x27; &lt;button v-show=&quot;!del_flag&quot;&gt;删除&lt;/button&gt;&lt;p&gt;&#123;&#123;content&#125;&#125;&lt;/p&gt;&lt;/div&gt;&#x27; , // 定义data类型返回数据的时候，最好使用一个自定义函数进行返回 data: function () &#123; return &#123;&#125; &#125;, methods: &#123; &#125; &#125;) // 创建一个Vue的实例 let vm = new Vue(&#123; // el 元素标签，绑定一个div el: &#x27;#app&#x27;, data: &#123; message: &#x27;这是一个Vue的组件demo&#x27;, list: [&#123; title: &#x27;标题1&#x27;, del_flag: false, content: &quot;测试对象绑定1&quot; &#125;,&#123; title: &#x27;标题2&#x27;, del_flag: true, content: &quot;测试对象绑定2&quot; &#125;] &#125; &#125;)&lt;/script&gt;&lt;/body&gt; 练习3 组件对象传入1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;body&gt;&lt;!-- 引入CDN 文件--&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/vue@2.6.14/dist/vue.js&quot;&gt;&lt;/script&gt;&lt;div id=&quot;app&quot;&gt; &lt;div&gt;&#123;&#123; message&#125;&#125;&lt;/div&gt; &lt;ul&gt; &lt;todo-item v-for=&quot; item in list &quot; v-bind:item=&quot;item&quot; &gt;&lt;/todo-item&gt; &lt;/ul&gt;&lt;/div&gt;&lt;script&gt; // todo-itme:表示组件的名称 // &#123;&#125;表示组件的对象 // template 表示组件的模板 Vue.component(&#x27;todo-item&#x27;,&#123; // 为了进行复用,如果变量有很多的时候,可知直接传入一个变量,取名叫做item props:[&#x27;item&#x27;], // 注意：组件模板必须包含一个根元素，否则控制台会报错 template:&#x27;&lt;div&gt;&lt;/iv&gt; &lt;span v-if=&quot;!item.del_flag&quot;&gt;&#123;&#123;item.title &#125;&#125;&lt;/span&gt;\\n&#x27; + &#x27; &lt;span v-else style=&quot;text-decoration: line-through;&quot;&gt;&#123;&#123;item.title &#125;&#125;&lt;/span&gt;\\n&#x27; + &#x27; &lt;button v-show=&quot;!item.del_flag&quot;&gt;删除&lt;/button&gt;&lt;p&gt;&#123;&#123;item.content&#125;&#125;&lt;/p&gt;&lt;/div&gt;&#x27; , // 定义data类型返回数据的时候，最好使用一个自定义函数进行返回 data: function () &#123; return &#123;&#125; &#125;, methods: &#123; &#125; &#125;) // 创建一个Vue的实例 let vm = new Vue(&#123; // el 元素标签，绑定一个div el: &#x27;#app&#x27;, data: &#123; message: &#x27;这是一个Vue的组件demo&#x27;, list: [&#123; title: &#x27;标题1&#x27;, del_flag: false, content: &quot;测试对象绑定1&quot; &#125;,&#123; title: &#x27;标题2&#x27;, del_flag: true, content: &quot;测试对象绑定2&quot; &#125;] &#125; &#125;)&lt;/script&gt; 注意 组件模板需要有,组件模板必须包含一个根元素，否则控制台会报错.Vue 会显示一个错误，并解释道 every component must have a single root element (每个组件必须只有一个根元素)。你可以将模板的内容包裹在一个父元素内，来修复这个问题 组件之间可以相互包含 定义data类型返回数据的时候，最好使用一个自定义函数进行返回,增加复用性 组件定义后之后，在使用的地方，可以直接编写组件的标签进行使用 练习2 和练习3 根据参数的传入方式不一样。属性props 和模板template 接收参数的格式也不一样","categories":[],"tags":[{"name":"前端框架","slug":"前端框架","permalink":"https://lswisdom.github.io/tags/%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6/"}]},{"title":"Es简介","slug":"Es的学习文档/01ES的简介","date":"2021-08-18T16:09:14.000Z","updated":"2021-11-24T00:28:28.640Z","comments":true,"path":"posts/221454123/","link":"","permalink":"https://lswisdom.github.io/posts/221454123/","excerpt":"","text":"ElasticSearch的简介 注意： 本文及接下来的文章中,我使用的ElasticSearch是7.12.0 的版本，ElasticSearch7 和 ElasticSearch6 是有些区别的，没有了type的概念，语法使用上有一定的区别 7.12.0 官方文档的地址：https://www.elastic.co/guide/en/elasticsearch/reference/7.12/index.html 7.12.0 kibanahttps://www.elastic.co/guide/en/kibana/7.12/kuery-query.html Rest API 地址：https://www.elastic.co/guide/en/elasticsearch/reference/7.12/rest-apis.html springboot data elasticsearch ： https://spring.io/projects/spring-data-elasticsearch 1.ElasticSearch基本概念1.1 ElasticSearch 是什么Elasticsearch 是一个分布式、RESTful 风格 的搜索和数据分析引擎，能够解决不断涌现出的各种用例。 是 Elastic Stack 的核心。 Logstash 和Beats 有助于收集、聚合和丰富您的数据并将其存储在 Elasticsearch 中. Kibana 使您能够以交互方式探索、可视化和共享对数据的见解，并管理和监控堆栈。 Elasticsearch 为所有类型的数据提供近乎实时的搜索和分析。无论您拥有结构化或非结构化文本、数值数据还是地理空间数据，Elasticsearch 都可以以支持快速搜索的方式高效地存储和索引它 1.2 ElasticSearch 能用来干什么 免费开放的日志监测。Elastic Stack（旧称 ELK Stack）是深受欢迎的免费开放日志平台。将您的数据索引到 Elasticsearch 中并在 Kibana 中进行可视化，分分钟搞定。 Elastic Metrics ： 开源基础架构监测。 APM : 精确查看您的应用程序都在哪里耗时了，然后便可快速修复问题， Elastic Uptime ：免费开源的运行状态监测 Elastic Site Search ： 轻松爬取网站内容并实现搜索功能 ELASTIC MAPS：使用 Elastic 地图分析您的地理空间数据。对多个索引进行可视化并将其作为单一视图中的特有图层，方便您查询并关联自己 Elasticsearch 中的全部数据。 SIEM : 对不断变化的威胁进行检测、调查和响应。利用云规模级的所有数据源。在主机层实现更好的控制。实施现代化安全用例，并迅速扩展。通过免费且开放的 Elastic 安全提高运营成熟度。 Elastic App Search : 轻松实现高级搜索 Elastic Workplace Search: Endpoint security : Elastic 安全可阻止恶意软件和勒索软件，推动集中式搜寻和检测，并支持交互式响应。 上面是官网介绍的一些功能,有兴趣同学，可以自己百度","categories":[],"tags":[{"name":"Es学习","slug":"Es学习","permalink":"https://lswisdom.github.io/tags/Es%E5%AD%A6%E4%B9%A0/"}]},{"title":"ElasticSearch索引学习","slug":"Es的学习文档/02Es的索引使用","date":"2021-08-18T16:09:14.000Z","updated":"2021-11-24T00:28:52.197Z","comments":true,"path":"posts/221453123/","link":"","permalink":"https://lswisdom.github.io/posts/221453123/","excerpt":"","text":"ElasticSearch索引学习作者：李帅 [TOC] 1.索引概念2 索引基本操作 文档地址：https://www.elastic.co/guide/en/elasticsearch/reference/current/indices.html?baymax=rec&amp;rogue=rec-1&amp;elektra=guide 2.1 创建索引Elasticsearch采用Rest风格API，因此其API就是一次http请求，你可以用任何工具发起http请求 请求分为 PUT POST GET DELETE GET : 查询数据 POST : 插入数据,也可以实现查询,查询条件复杂时,参数可以使用json PUT : 更新数据,实际上很多情况下 es 不是很清晰你到底要作什么,所有有时候添加也需要使用 PUT 请求,如果错了,会告诉我请求方式不对 DELETE : 删除数据 2.1.1创建索引的请求格式 请求方式：PUT 请求路径：/索引库名 请求参数：json格式 索引的格式要求 **&lt;index&gt;**（必需，字符串）要创建的索引的名称。 索引名称必须满足以下条件： 仅小写 不能包含\\, /, *, ?, &quot;, &lt;, &gt;, |, （空格字符）, ,,# 7.0 之前的索引可以包含冒号 ( :)，但它已被弃用并且在 7.0+ 中不受支持 不能以-, _,开头+ 不能是.或.. 不能超过 255 个字节（注意它是字节，因此多字节字符将更快地计入 255 个限制） .不推荐使用以 开头的名称，插件管理的隐藏索引和内部索引 除外 mappings （可选，映射对象）索引中字段的映射。如果指定，此映射可以包括： 字段名称 字段数据类型 映射参数 请参阅映射。 settings （可选，索引设置对象）索引的配置选项。请参阅索引设置。 其他略 2.1.2 创建索引的请求Demo直接创建索引 12# 创建索引PUT /ls_person 执行结果如下: 12345&#123; &quot;acknowledged&quot; : true, # 表示是否在集群中成功创建索引 &quot;shards_acknowledged&quot; : true, # 表示是否在超时之前为索引中的每个分片启动了所需数量的分片副本 &quot;index&quot; : &quot;ls_person&quot; # 索引的日志&#125; 出现如下,表示索引创建成功了 创建索引时设置settings 12345678910# 创建索引时添加setting 设置PUT /ls_person_two&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; # index 可以不写 &quot;number_of_replicas&quot;: 1, # 默认的备份数,也是 1 &quot;number_of_shards&quot;: 1 # 分片数默认为1 &#125; &#125;&#125; 创建索引时指定索引的结构mapping 123456789101112131415161718192021#创建索引的映射PUT /ls_person_three?pretty&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;birthday&quot;:&#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: [&quot;yyyy-MM-dd&quot;] &#125; &#125; &#125;&#125; 在kibana中查看结构如下： **创建索引时允许提供别名 ** 2.1.3 创建索引的Java代码演示1234567891011121314151617181920212223242526public class ESCreateIndexDemo &#123; private static final Logger logger = Logger.getLogger(ESCreateIndexDemo.class); /** 索引名称 */ public static final String index = &quot;ls_person_five811&quot;; /** * 创建索引测试类 */ @Test public void testCreateIndex() throws IOException &#123; // 1.创建ES高级客户端 HttpHost httpHost = new HttpHost(&quot;192.168.154.129&quot;, 9200); RestClientBuilder restClientBuilder = RestClient.builder(httpHost); RestHighLevelClient client = new RestHighLevelClient(restClientBuilder); // 2.创建索引请求 CreateIndexRequest createIndexRequest = new CreateIndexRequest(index); // 参数1表示创建索引的请求 . CreateIndexResponse response = client.indices().create(createIndexRequest, RequestOptions.DEFAULT); // 如果创建的索引存在,会抛出索引已经存在的异常 // 结果为true 表示创建成功了 logger.debug(&quot;创建结果:&quot; + response.isAcknowledged()); &#125; 2.2 删除索引删除索引使用DELETE请求 语法 1DELETE &#x2F;索引库名 注意删除索引会删除其文档、分片和元数据 &lt;index&gt; （必需，字符串）要删除的索引的逗号分隔列表。 要删除所有索引，请使用_all或*。要禁止删除带有_all或 通配符表达式的索引，请将 action.destructive_requires_name集群设置设置为true。 设置为true时，表示您必须指定索引名称才能删除索引。无法_all使用通配符删除所有索引。 2.2.1 删除索引的请求Demo如果要删除多个索引,中间使用都好隔开 1DELETE /ls_person_three,ls_person_five,ls_person_three 如下图,可以看到刚才创建的索引已经删除了 2.2.2 删除索引通过Java代码方式12345678910111213141516/** * 删除索引 */@Testpublic void deleteIndexTest() throws IOException &#123; //1.创建Rest高级客户端 HttpHost httpHost = new HttpHost(&quot;192.168.154.129&quot;, 9200); RestClientBuilder restClientBuilder = RestClient.builder(httpHost); RestHighLevelClient client = new RestHighLevelClient(restClientBuilder); // 2.创建删除索引请求 DeleteIndexRequest deleteIndexRequest = new DeleteIndexRequest(&quot;ls_person&quot;, &quot;person&quot;); AcknowledgedResponse response = client.indices().delete(deleteIndexRequest, RequestOptions.DEFAULT); // 执行结果为Ture表示创建成功 如果索引不存在,会抛出异常ElasticsearchStatusException logger.info(&quot;执行结果：&quot; + response.isAcknowledged());&#125; 删除后在控制台查看如下： 由此,可以看到,索引已经删除成功了 2.3 获取索引Get 请求可以帮我们查看索引信息 1GET &#x2F;索引名称 支持查询多个，索引和别名的逗号分隔列表。支持通配符 *或_all。 2.3.1 获取索引的请求Demo1234567891011# 获取单个索引GET ls_person1# 获取多个索引,中间使用逗号隔开GET ls_person1,ls_person2#使用通配符获取索引GET user*# 获取全部索引GET _all 2.3.2 获取索引的代码实现12345678910111213@Testpublic void getIndexTest() throws IOException &#123; //1.创建Rest高级客户端 HttpHost httpHost = new HttpHost(&quot;192.168.154.129&quot;, 9200); RestClientBuilder restClientBuilder = RestClient.builder(httpHost); RestHighLevelClient client = new RestHighLevelClient(restClientBuilder); GetIndexRequest getIndexRequest = new GetIndexRequest(&quot;ls_person1&quot;,&quot;ls_person2&quot;); GetIndexResponse response = client.indices().get(getIndexRequest, RequestOptions.DEFAULT); String[] indices = response.getIndices(); for (String indexName : indices) &#123; logger.info(&quot;查询出的索引名称：&quot; + indexName); &#125;&#125; 2.4 检查索引是否存在检查数据、索引或别名是否存在。多个之间使用逗号隔开 1HEAD &lt;target&gt; 2.4.1 检查索引是否存在的请求Demo12# 检查索引是否存在HEAD ls_person1 返回值为200 表示存在 404 表示不存在,返回值格式不一样 如下所示： 12200 - OK # 存在&#123;&quot;statusCode&quot;:404,&quot;error&quot;:&quot;Not Found&quot;,&quot;message&quot;:&quot;404 - Not Found&quot;&#125; # 不存在,如果是多个索引,有一个不存在,结果就是404 2.4.2 检查索引是否存在的请求JAVA实现1234567891011121314/** * 检查索引是否存在 */@Testpublic void checkIndexExist() throws IOException &#123; //1.创建Rest高级客户端 HttpHost httpHost = new HttpHost(&quot;192.168.154.129&quot;, 9200); RestClientBuilder restClientBuilder = RestClient.builder(httpHost); RestHighLevelClient client = new RestHighLevelClient(restClientBuilder); GetIndexRequest getIndexRequest = new GetIndexRequest(&quot;ls_person1&quot;, &quot;ls_person1&quot;); boolean exists = client.indices().exists(getIndexRequest, RequestOptions.DEFAULT); // 结果为ture表示存在,为false表示不存在 logger.info(&quot;索引是否存在结果：&quot; + exists);&#125; 2.5 关闭索引API: 作用：不允许被关闭的索引进行任何的查询或者写入操作。无法索引文档或在封闭索引中搜索文档。这允许封闭索引不必维护用于索引或搜索文档的内部数据结构，从而减少集群上的开销。 1POST &#x2F; my-index-000001 &#x2F; _close 封闭索引会消耗大量磁盘空间，这可能会导致托管环境出现问题。关闭索引可以通过集群设置 API 设置cluster.indices.close.enable为来禁用false。默认值为true. 2.6 打开索引1POST &#x2F; my-index-000001 &#x2F; _open 关闭索引被阻止进行读/写操作，并且不允许打开索引允许的所有操作。无法索引文档或在封闭索引中搜索文档。这允许封闭索引不必维护用于索引或搜索文档的内部数据结构，从而减少集群上的开销。 在打开或关闭索引时，master 负责重新启动索引分片以反映索引的新状态。然后分片将经历正常的恢复过程。打开/关闭索引的数据由集群自动复制，以确保始终安全地保留足够的分片副本。 2.7 其他操作关于索引官网上还有一些其他操作，有兴趣的可以自行查看索引的API API地址如下：https://www.elastic.co/guide/en/elasticsearch/reference/current/indices.html?baymax=rec&amp;rogue=rec-1&amp;elektra=guide 3 索引字段映射3.1 创建索引时指定字段映射3.1.1 创建索引映射Demo123456789101112131415161718192021#创建索引的映射PUT /ls_person?pretty&#123; &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 1 &#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;: &quot;integer&quot; &#125;, &quot;birthday&quot;:&#123; &quot;type&quot;: &quot;date&quot;, &quot;format&quot;: [&quot;yyyy-MM-dd&quot;] &#125; &#125; &#125;&#125; 3.1.2 创建索引映射Demo的Java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * 创建索引和指定索引的映射 * PUT /ls_person20?pretty * &#123; * &quot;settings&quot;: &#123; * &quot;number_of_shards&quot;: 1 * &#125;, * &quot;mappings&quot;: &#123; * &quot;properties&quot;: &#123; * &quot;name&quot;: &#123; * &quot;type&quot;: &quot;text&quot; * &#125;, * &quot;age&quot;:&#123; * &quot;type&quot;: &quot;integer&quot; * &#125;, * &quot;birthday&quot;:&#123; * &quot;type&quot;: &quot;date&quot;, * &quot;format&quot;: [&quot;yyyy-MM-dd&quot;] * &#125; * &#125; * &#125; * &#125; */@Testpublic void createIndexAndMapping() throws IOException &#123; //1.创建Rest高级客户端 HttpHost httpHost = new HttpHost(&quot;192.168.154.129&quot;, 9200); RestClientBuilder restClientBuilder = RestClient.builder(httpHost); RestHighLevelClient client = new RestHighLevelClient(restClientBuilder); // 2.准备需要创建的索引的映射 Settings.Builder settings = Settings.builder().put(&quot;number_of_shards&quot;,1); XContentBuilder xContentBuilder = JsonXContent.contentBuilder() // 字段需要一一映射，开始和结束。这里可以认为表示括号&#123; endObject表示&#125; .startObject() .startObject(&quot;properties&quot;) .startObject(&quot;name&quot;) .field(&quot;type&quot;, &quot;text&quot;) .endObject() .startObject(&quot;age&quot;) // value的值不要写错了，否则会抛出 ElasticsearchException[Elasticsearch exception [type=mapper_parsing_exception, reason=No handler for type [Integer] declared on field [age]]]; .field(&quot;type&quot;, &quot;integer&quot;) .endObject() .startObject(&quot;birthday&quot;) .field(&quot;type&quot;, &quot;date&quot;) .field(&quot;format&quot;, &quot;yyyy-MM-dd&quot;) .endObject() .endObject() .endObject(); // 创建索引 CreateIndexRequest createIndexRequest = new CreateIndexRequest(&quot;ls_person_mapping&quot;); createIndexRequest.settings(settings) .mapping(xContentBuilder); // 通过客户端创建请求 CreateIndexResponse response = client.indices().create(createIndexRequest, RequestOptions.DEFAULT); logger.info(&quot;索引创建结果：&quot; + response.isAcknowledged());&#125; kibana运行截图如下： 由此，可见，索引的映射创建成功了 3.2 获取索引字段映射3.2.1 获取索引映射请求语法 获取一个或者多个索引的映射信息 12345678910GET /&lt;target&gt;/_mappingGET /索引名称(多个之间使用逗号隔开)/_mapping# 获取多个索引,中间使用逗号隔开GET /ls_person,ls_person20/_mapping# 所有的索引映射GET /*/_mapping# 所有的索引映射GET /_all/_mapping # 所有的索引映射GET /_mapping &lt;target&gt; （可选，字符串）用于限制请求的数据流、索引和别名的逗号分隔列表。支持通配符 *或_all。 执行结果如下： 1234567891011121314151617181920212223242526272829303132333435&#123; &quot;ls_person20&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;age&quot; : &#123; &quot;type&quot; : &quot;integer&quot; &#125;, &quot;birthday&quot; : &#123; &quot;type&quot; : &quot;date&quot;, &quot;format&quot; : &quot;[yyyy-MM-dd]&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125; &#125; &#125; &#125;, &quot;ls_person&quot; : &#123; &quot;mappings&quot; : &#123; &quot;properties&quot; : &#123; &quot;age&quot; : &#123; &quot;type&quot; : &quot;integer&quot; &#125;, &quot;birthday&quot; : &#123; &quot;type&quot; : &quot;date&quot;, &quot;format&quot; : &quot;[yyyy-MM-dd]&quot; &#125;, &quot;name&quot; : &#123; &quot;type&quot; : &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 3.2.2 获取索引映射Java代码实现123456789101112131415161718/** * 获取索引映射的数据 * @throws IOException */@Testpublic void getIndexMappingTest() throws IOException &#123; //1.创建Rest高级客户端 HttpHost httpHost = new HttpHost(&quot;192.168.154.129&quot;, 9200); RestClientBuilder restClientBuilder = RestClient.builder(httpHost); RestHighLevelClient client = new RestHighLevelClient(restClientBuilder); GetIndexRequest getIndexRequest = new GetIndexRequest(&quot;ls_person_mapping&quot;); GetIndexResponse response = client.indices().get(getIndexRequest, RequestOptions.DEFAULT); String indexName = &quot;ls_person_mapping&quot;; MappingMetadata metaData = response.getMappings().get(indexName); logger.info(&quot;索引名称：&quot; + indexName); logger.info(&quot;索引映射数据&quot; + metaData.getSourceAsMap());&#125; 执行结果如下： 12索引名称：ls_person_mapping索引映射数据&#123;properties&#x3D;&#123;birthday&#x3D;&#123;format&#x3D;yyyy-MM-dd, type&#x3D;date&#125;, name&#x3D;&#123;type&#x3D;text&#125;, age&#x3D;&#123;type&#x3D;integer&#125;&#125;&#125; 3.3 获取索引Mappping字段映射3.3.1 获取索引Mapping的字段映射Demo定义 检索一个或多个字段的映射定义。对于数据流，API 检索流的支持索引的字段映射。 语法 1234567GET &#x2F;_mapping&#x2F;field&#x2F;&lt;field&gt;GET &#x2F;&lt;target&gt;&#x2F;_mapping&#x2F;field&#x2F;&lt;field&gt;&lt;target&gt;（可选，字符串）用于限制请求的数据流、索引和别名的逗号分隔列表。支持通配符 ( *)。要定位所有数据流和索引，请省略此参数或使用*或_all。&lt;field&gt;（可选，字符串）用于限制返回信息的字段的逗号分隔列表或通配符表达式 例子 1234567891011121314151617181920212223242526272829303132333435363738PUT /publications?pretty&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;title&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;abstract&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;author&quot;: &#123; &quot;properties&quot;: &#123; &quot;id&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125;, &quot;name&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125; &#125;&#125;#获取索引Mapping的字段映射GET ls_person20/_mapping/field/birthday# get mapping API 允许您指定以逗号分隔的字段列表。GET /publications/_mapping/field/author.id,abstract,name?pretty# 使用通配符形式的字段映射GET publications/_mapping/field/a*# 上述&lt;target&gt;和&lt;field&gt;请求路径参数都支持逗号分隔的列表和通配符表达式。 这里的ls_person是我上文中创建的索引对象GET ls_person,ls_person20/_mapping/field/name,birthday 3.3.2 获取索引Mapping的字段映射Java实现3.4 更新索引Mapping映射定义 ：向现有数据流或索引添加新字段 语法： 123PUT &#x2F;&lt;target&gt;&#x2F;_mapping&lt;target&gt;（必需，字符串）用于限制请求的数据流、索引和别名的逗号分隔列表。支持通配符 ( *)。要定位所有数据流和索引，请省略此参数或使用*或_all。 3.4.1 更新索引Mapping映射DEMO单个索引属性更新 123456789101112131415# 创建索引PUT /ls_person30# 更新索引字段映射PUT /ls_person30/_mapping&#123; &quot;properties&quot;:&#123; &quot;name&quot;:&#123; &quot;type&quot;:&quot;keyword&quot; &#125;, &quot;age&quot;:&#123; &quot;type&quot;: &quot;integer&quot; &#125; &#125;&#125; 多个索引更新 更新映射 API 可以通过单个请求应用于多个索引 12345678910111213141516171819# Create the two indicesPUT /ls_person40PUT /ls_person41# Update both mappingsPUT /ls_person40,ls_person41/_mapping&#123; &quot;properties&quot;: &#123; &quot;user&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125;GET ls_person40,ls_person41/_mapping 向现有的字段对象中添加新的属性 123456789101112131415161718192021222324252627282930PUT &#x2F;ls_person50?pretty&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;properties&quot;: &#123; &quot;first&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125; &#125;&#125;get ls_person50# 向name字段中添加新的字段lastPUT &#x2F;ls_person50&#x2F;_mapping?pretty&#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;properties&quot;: &#123; &quot;last&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125; &#125;&#125; 将多字段添加到现有字段中 12345678910111213141516171819202122232425PUT /ls_person60?pretty&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;city&quot;: &#123; &quot;type&quot;: &quot;text&quot; &#125; &#125; &#125;&#125;get ls_person60PUT /ls_person60/_mapping?pretty&#123; &quot;properties&quot;: &#123; &quot;city&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;fields&quot;: &#123; &quot;raw&quot;: &#123; &quot;type&quot;: &quot;keyword&quot; &#125; &#125; &#125; &#125;&#125; 3.4.2 更新索引Mapping映射Java实现略 3.5 检查索引类型是否存在 ElasticSearch7.0 之后，索引类型就已经弃用了 https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-types-exists.html 3.6 索引使用情况的API https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-disk-usage.html 定义：分析索引或数据流的每个字段的磁盘使用情况 语法 : 有一些可选参数。实际测试没有得到我想要的结果，有兴趣的同学可以在上面的官网中查看和学习 1POST &#x2F;&lt;target&gt;&#x2F;_disk_usage 4.索引设置4.1 获取索引设置作用 ： 返回一个或多个索引的设置信息. 格式 123GET &#x2F;&lt;target&gt;&#x2F;_settingsGET &#x2F;&lt;target&gt;&#x2F;_settings&#x2F;&lt;setting&gt; 要获取集群中所有索引的设置.可以使用_all或*for &lt;target&gt;。还支持通配符表达式 例如： 1234# 中间不要有空格,否则会提示 illegal_argument_exception request [GET /ls_person50,ls_person40] does not support having a bodyGET /ls_person50,ls_person40/_settingsGET /_all/_settings GET /ls_person*/_settings 4.2 索引分析文本 https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html 作用：对文本字符串执行分析并返回结果标记。 格式： 1234567GET &#x2F;_analyzePOST &#x2F;_analyzeGET &#x2F;&lt;index&gt;&#x2F;_analyzePOST &#x2F;&lt;index&gt;&#x2F;_analyze 执行例子 1234567891011121314151617181920212223# 索引分析操作 standard如果没有指定，分析器是默认的分析器。GET /_analyze?pretty&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;小河流水哗啦啦&quot;&#125;# 采用ik分词器的分词 需要自行安装GET /_analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;小河流水哗啦啦&quot;&#125;# 文本字符串数组则将其作为多值字段进行分析GET /ls_person/_analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: [ &quot;小河流水哗啦啦&quot;, &quot;我和姐姐去彩花t&quot; ]&#125; analyzer 应用于提供的text. 这可以是内置的分析器，也可以是 在索引中配置的分析器。 如果未指定此参数，则分析 API 使用字段映射中定义的分析器。如果未指定字段，则分析 API 使用索引的默认分析器。 如果未指定索引，或者索引没有默认分析器，则分析 API 使用标准分析器 standard。 4.3 IK分词器安装Lucene的IK分词器的很早就不维护了，索引现在我们要使用的是在其基础上维护升级的版本。因为我所使用的ElasticSearch是7.12.0版本，所以我的IK分词器也下载对应的7.12.0的版本 IK分词器安装地址:https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v7.12.0 ik分词器，解压在elasticsearch的plugin目录下面。然后重新启动elasticsearch就可以使用了 例如： 1&#x2F;usr&#x2F;local&#x2F;software&#x2F;elasticsearch-7.12.0&#x2F;plugins 4.4 更新索引设置作用： 实时更改动态索引设置。 执行例子 123456PUT /ls_person/_settings?pretty&#123; &quot;index&quot;: &#123; &quot;number_of_replicas&quot;: 3 &#125;&#125; 6.索引的别名管理略 有兴趣的同学自行查看ES的API 7. 索引模板有兴趣的同学自行查看ES的API 8. 索引监控8.1 索引统计定义 : 获取一个或多个数据流和索引的高级聚合和统计信息。 格式 12345GET &#x2F;&lt;target&gt;&#x2F;_stats&#x2F;&lt;index-metric&gt;GET &#x2F;&lt;target&gt;&#x2F;_statsGET &#x2F;_stats 默认情况下，返回的统计信息是索引级别的primaries和total聚合。 primaries是仅主分片的值。 total是主分片和副本分片的累计值。 要获取分片级别的统计信息，请将level参数设置为shards。 执行例子 1234567# 获取多个索引的统计信息 多个之间使用逗号隔开GET /ls_person,ls_person2/_stats # 获取集群中所有的索引的统计信息GET /_statsGET /_stats/merge,refresh?prettyGET /ls_person30/_stats/merge,refresh?pretty 8.2 索引字段统计定义 : 返回索引的每个分片和字段的字段使用信息。当查询在集群上运行时，会自动捕获字段使用统计信息。访问给定字段的分片级搜索请求，即使在该请求期间多次，也计为一次使用。 格式 1GET &#x2F;&lt;index&gt;&#x2F;_field_usage_stats 执行例子 略。未来版本 该功能可能会删除。官网提供的查询例子貌似有问题 8.3 索引分片存储略 定义： 一个或多个索引中检索有关副本分片的存储信息 格式： 123GET &#x2F;&lt;target&gt;&#x2F;_shard_storesGET &#x2F;_shard_stores 8.4 索引恢复略 定义：返回有关一个或多个索引的正在进行和已完成的分片恢复的信息 格式： 123GET &#x2F;&lt;target&gt;&#x2F;_recoveryGET &#x2F;_recovery 9.索引状态管理9.1 清除索引的缓存定义：清除一个或多个索引的缓存 格式： 123POST &#x2F;&lt;target&gt;&#x2F;_cache&#x2F;clearPOST &#x2F;_cache&#x2F;clear 支持通配符 ( *)。要定位所有数据流和索引，请省略此参数或使用*或_all。 清除特定缓存 默认情况下，清除缓存 API 会清除所有缓存。您可以通过将以下查询参数设置为来仅清除特定缓存true 1234567891011# 清除特定缓存# 仅清除字段缓存POST /ls_person/_cache/clear?fielddata=true&amp;pretty# 只清除查询缓存POST /ls_person/_cache/clear?query=true&amp;pretty# 只清除请求缓存POST /ls_person/_cache/clear?request=true&amp;pretty# 不指定索引清除所有的缓存POST /_cache/clear?pretty# 清除多个索引的缓存 多个之间使用逗号隔开POST /ls_person,ls_person2/_cache/clear?pretty 9.2 索引刷新 近实时搜索：https://www.elastic.co/guide/en/elasticsearch/reference/current/near-real-time.html 定义： 对一个或者多个索引进行刷新操作。可用于搜索 使用刷新 API 显式地使自上次刷新以来对一个或多个索引 执行的所有操作可用于搜索。 默认情况下，Elasticsearch 每秒都会定期刷新索引，但仅限于在过去 30 秒内收到一个或多个搜索请求的索引。 您可以使用index.refresh_interval设置更改此默认间隔。 刷新请求是同步的，在刷新操作完成之前不会返回响应。 格式： 1234567POST &lt;target&gt;&#x2F;_refreshGET &lt;target&gt;&#x2F;_refreshPOST &#x2F;_refreshGET &#x2F;_refresh 使用例子 123456# 刷新一个或者多个索引POST /ls_person,ls_person2/_refresh?pretty# 刷新所有的索引POST /_refresh 注意 刷新是资源密集型的。为了确保良好的集群性能，我们建议尽可能等待 Elasticsearch 的定期刷新，而不是执行显式刷新。 9.3 刷新定义： 刷新一个或多个索引 ​ 刷新数据流或索引是确保当前仅存储在事务日志中的任何数据也永久存储在 Lucene 索引中的过程。 重新启动时，Elasticsearch 会将所有未刷新的操作从事务日志中重放到 Lucene 索引中，以将其恢复到重新启动前的状态。Elasticsearch 会根据需要自动触发刷新. 一旦每个操作被刷新，它就会永久存储在 Lucene 索引中。这可能意味着不需要在事务日志中维护它的额外副本，除非它因某些其他原因被保留。事务日志由多个文件组成，称为generation，一旦不再需要，Elasticsearch 将删除任何生成文件，从而释放磁盘空间。 格式： 1234567POST &#x2F;&lt;target&gt;&#x2F;_flushGET &#x2F;&lt;target&gt;&#x2F;_flushPOST &#x2F;_flushGET &#x2F;_flush 使用例子 12345# 刷新特定索引或者数据。支持多个,中间使用逗号隔开POST /ls_person,ls_person2/_flush?pretty# 刷新所有的索引和数据流POST /_flush 9.4 强制合并一个或多个索引的分片略","categories":[],"tags":[{"name":"Es学习","slug":"Es学习","permalink":"https://lswisdom.github.io/tags/Es%E5%AD%A6%E4%B9%A0/"}]},{"title":"线程与并发基础","slug":"Java基础/线程与并发/01线程与并发基础","date":"2021-08-18T16:09:14.000Z","updated":"2021-11-24T00:26:20.290Z","comments":true,"path":"posts/221454721/","link":"","permalink":"https://lswisdom.github.io/posts/221454721/","excerpt":"","text":"01 线程与并发基础线程概念线程： 线程是CPU分配的基本单位 进程 ： 进程是进行资源分配和调度的基本单位。 进程与线程关系：线程是进程的一个执行路径 ，一个进程中至少包含一个线程，进程中的多个线程共享进程中的资源。 同步和异步同步和异步 通常用来形容一次方法调用。 同步： 同步方法一旦调用开始，调用者必须等到方法调用返回后，才能继续后续的行为。 异步： 异步方法调用更像一个消息传递，一旦开始，方法调用会立即返回，调用者才能继续后续操作。异常方法通常会在另外一个线程中执行。 临界区临界区 ：临界区用来表示一种公共资源或者说是共享数据，可以被多个线程使用，但是每一次，都只能有一个线程来使用它，一旦临界资源被占用，其他线程想要使用这个资源，就必须进行等待。在并行程序中，临界资源是需要保护的资源。 线程定义多线程间的相互影响线程阻塞(Blocking) : 一个线程占用了临界区的资源，那么其他所有需要这个资源的线程就必须在这个临界区中等待。等待会导致线程挂起，这种情况被称为线程阻塞。如果占用资源的线程一直不愿意释放资源，那么其他阻塞在这个临界区的线程都不能工作。 线程非阻塞(Non-Blocking) ：没有一个线程可以妨碍其他线程的执行。所有的线程都会尝试不断执行。 多线程的活跃性如果出现死锁、饥饿、活锁中的任何一种，那么线程可能就不再活跃了，很难执行下去了。 线程死锁(DeadLock) ： 线程死锁是指两个或两个以上的线程互相持有对方所需要的资源，由于synchronized的特性，一个线程持有一个资源，或者说获得一个锁，在该线程释放这个锁之前，其它线程是获取不到这个锁的，而且会一直死等下去，因此这便造成了死锁。 线程饥饿(Starvation) ： 饥饿是指某一个或者多个线程由于种种原因无法获得所需要的资源，导致一直无法执行。 情况一 ：例如它的线程优先级可能非常低，而高优先级的线程不断抢占它需要的资源，导致低优先级的工作无法执行。 情况二 ：某一个线程一直占着关键资源不放手，导致其他需要这个资源的线程无法执行，这种情况也是线程饥饿的一种。 线程饥饿还是有可能在未来的某一段时间内解决的。比如高优先级的线程已经执行完，不再疯狂的创建线程。 线程活锁(LiveLock) ：是指多个线程秉承着”谦让的”原则，主动将资源释放给其他人使用，那么就会出现资源在两个线程中不断的跳动，而没有一个线程可以同时拿到所有的资源而正常执行。这种情况就是活锁 并发级别由于临界区的存在,多线程之间的并发控制必须受到控制。并发的级别分为五大类。阻塞 、无饥饿 、无障碍 、无锁 、无等待 阻塞 ： 一个线程是阻塞的，那么其他线程释放资源之前，当前线程无法继续执行。当我们使用synchronized关键字或者重入锁的时候，我们得到的就是阻塞的线程。 无饥饿 ：如果线程之间是有优先级的，那么线程的调用总是会倾向于高优先级的线程。也就是说，对于一个资源的分配是不公平的。对于非公平的锁来说，系统允许高优先级的线程插队。这样有可能导致低优先级的线程产生饥饿 .但是如果锁是公平的，那么饥饿就不会产生，不管新来的线程优先级有多高，要想获取到资源，就必须排队，所有的线程都会有机会执行。","categories":[],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://lswisdom.github.io/tags/Java%E5%9F%BA%E7%A1%80/"}]},{"title":"HashMap简单讲解","slug":"Java基础/HashMap简单讲解","date":"2021-08-17T16:09:14.000Z","updated":"2021-08-22T23:53:21.838Z","comments":true,"path":"posts/221454777/","link":"","permalink":"https://lswisdom.github.io/posts/221454777/","excerpt":"","text":"JDK1.7 HashMapJDK1.8 HashMap基本信息描述JDK1.7采用 数组+链表 的数据结构JDK1.8的HashMap采用 数组+链表+红黑树 的数据结构。新增加红黑树的操作,是为了解决某些情况下,链表过长导致的查询效率问题。链表查询数据的时间复杂度为O(n),红黑树的时间复杂度为Olog(n).当数据量多的时候,红黑树的查询效率明显高于链表数据结构如下： 基本字段属性123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; private static final long serialVersionUID = 362498820763181265L; /** * 集合默认的容量是16,大小必须是2的幂次方 */ static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 /** * 集合的最大容量。初始化或者扩容时,防止溢出,必须是2次幂 &lt;= 1&lt;&lt;30. */ static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; /** * 构造函数中未指定负载因子时,使用该值,默认是0.75,不建议修改,该值经过大量计算得出.遵循泊松分布 */ static final float DEFAULT_LOAD_FACTOR = 0.75f; /** * 桶的树化阈值 当链表长度&gt;=8 时,将链表转换成红黑树 */ static final int TREEIFY_THRESHOLD = 8; /** * 红黑树还原回链表的阈值。当红黑树节点数量&lt;=6时,将红黑树转换为链表结构 * 基于时间和空间的考虑 */ static final int UNTREEIFY_THRESHOLD = 6; /** * 桶树化的最小表容量： 当桶中的元素大于64时,且链表的容量&gt;=TREEIFY_THRESHOLD 时,才可以把链表转换为红黑树 * 设置成64,是为了减少扩容的次数。当桶比较小的时候,桶中的元素达到一定的个数时,会频繁的扩容操作。浪费性能 * 也是时间和空间的一种考虑 */ static final int MIN_TREEIFY_CAPACITY = 64; /* ---------------- Fields -------------- */ /** * 第一次使用时进行初始化操作。长度是2的幂 */ transient Node&lt;K,V&gt;[] table; /** * 缓存字段 */ transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; /** * table中元素的个数 */ transient int size; /** * hashmap改变的次数 */ transient int modCount; /** * 库容的阈值,大小等于 = (capacity * load factor). * * @serial */ int threshold; /** * 负载因子,默认是0.75 * * @serial */ final float loadFactor; 注意： TREEIFY_THRESHOLD=8 和 UNTREEIFY_THRESHOLD=6 是链表转红黑树和红黑树退化成链表的阈值。因为桶中链表的数量在计算hash值的时候,遵循泊松分布。当链表的长度为8时的概率为 8: 0.00000006 而当链表的长度为6时的概率为 6: 0.00001316 概率差了1000多倍。为了避免频繁的树化和解除树化的操作 桶中元素结构体123456789101112static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; // hash值 final K key; // key值 V value; // 桶中元素的value值 Node&lt;K,V&gt; next; // 桶中下一个元素的指针 Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; 链表树化结构体12345678910static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125;&#125; 构造方法123456789101112131415161718192021222324252627282930313233343536373839404142/** * * @param initialCapacity 初始容量 * @param loadFactor 负载因子 * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); // 如果初始容量大于最大容量2^30,赋值为最大容量。防止溢出 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; // threshold hashmap扩容阈值,注意这个值会发生变化,如果initialCapacity是2的幂,返回原值。如果不是2的幂,返回大于该值的最小的2的幂次方 this.threshold = tableSizeFor(initialCapacity); &#125; /** * 指定集合容量大小,默认负载因子是0.75 */ public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; /** * 使用默认属性 */ public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; /** */ public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; 注意： 集合容量设置过小,会造成频繁的扩容操作。设置容量过大,会导致空间上浪费。如果我们确切的知道我们有多少键值对需要存储,那么我们在初始化HashMap的时候就应该指定它的容量,防止HashMap自动扩容,影响使用效率。 initialCapacity = (需要存储的元素个数 / 负载因子) + 1 常用方法确定桶的位置1tab[i = (n - 1) &amp; hash hash方法1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 计算 key.hashCode() 并将散列的较高位（异或）传播到较低位。这个怎么理解呢?这个哈希方法首先计算出key的hashCode,然后赋值给h,然后与h无符号右移16位后的二进制进行按位异或得到最后的hash值(hashCode()计算方法,得出的是一个int类型数,转换为二进制也就是32位)。hash值计算完成后,需要用来计算元素在桶中的位置,计算公式:tab[i = (n - 1) &amp; hash 如果当桶很小时,假设是默认的16的话,这样的值和hashCode()直接做按位与操作,实际上只使用了哈希值的后4位。如果当哈希值的高位变化很大,低位变化很小,这样就很容易造成哈希冲突了,所以这里把高低位都利用起来,从而解决了这个问题。也是一种为了降低hash冲突的优化手段。举个例子如下： &amp; (按位与运算) : 运算规则:相同的二进制数位上,都是1的时候,结果为1,否则为零。 ^ (按位异或运算) :运算规则:相同的二进制数位上,数字相同,结果为0,不同为1. 代码中通过这个hash &amp; (n-1) 得到存储元素的位置,假设这里的h = key.hashCode()得到的值为,分别进行桶中元素位置计算,看看新老hash算法差异key1 = 0000 0000 0001 0000 1111 1111 0000 0000key2 = 0000 1111 1111 1111 1111 1111 0000 0000 高位变化较大 1234567891011121314151617181920212223242526272829303132333435使用h = key.hashCode()) ^ (h &gt;&gt;&gt; 16 获取hash值key1的计算如下：0000 0000 0001 0000 1111 1111 0000 0000 h0000 0000 0000 0000 0000 0000 0001 0000 h &gt;&gt;&gt; 16----------------------------------------------------------0000 0000 0001 0000 1111 1111 0001 0000 按位异或运算后得到的hash值 h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)此时数据长度n假设为默认的16,那么这个key存放在table中位置为i = (n - 1) &amp; hash0000 0000 0001 0000 1111 1111 0001 0000 hash0000 0000 0000 0000 0000 0000 0000 1111 n-1 15 &amp;运算----------------------------------------------------------0000 0000 0000 0000 0000 0000 0000 0000 索引位置为0key2的计算：0000 1111 1111 1111 1111 1111 0000 0000 h0000 0000 0000 0000 0000 1111 1111 1111 h &gt;&gt;&gt; 16-----------------------------------------------------------0000 1111 1111 1111 1111 0000 1111 1111 按位异或运算后得到的hash值 h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)0000 1111 1111 1111 1111 0000 1111 1111 0000 0000 0000 0000 0000 0000 0000 1111 n-1 15 &amp;运算------------------------------------------------------------0000 0000 0000 0000 0000 0000 0000 1111 索引位置是15 反例： (key.hashCode()) &amp; (n-1)key1:0000 0000 0001 0000 1111 1111 0000 0000 hash值(重新获取一个hash值,直接进行按位与操作)0000 0000 0000 0000 0000 0000 0000 1111 n-1 15 &amp;运算-------------------------------------------------------0000 0000 0000 0000 0000 0000 0000 0000 索引位置为0key2:0000 1111 1111 1111 1111 1111 0000 0000 hash值 高位变化较大的hash值,低位保持不变0000 0000 0000 0000 0000 0000 0000 1111 n-1 15 &amp;运算 ---------------------------------------------------------------0000 0000 0000 0000 0000 0000 0000 0000 索引位置为0 由此可见,同样的两个hashcode,经过移位和异或操作后,能够使hashcode更加复杂,同时也把哈希值的高位移动到了低位,降低了hash冲突的概率。实际上hashMap的hash算法做的非常好,进过我的测试,默认负载因子为0.75的时候如果把hashmap加入4000w个数据的时候。依然没有链表转红黑树的操作,都是不断扩容的操作.有兴趣的朋友,可以自己测试一下hash算法的性能。 扩容resize()源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * 初始化或者扩容表的长度为2倍 * @return the table */ final Node&lt;K,V&gt;[] resize() &#123; // 旧表,第一次执行时,oldTabl为空 Node&lt;K,V&gt;[] oldTab = table; // 旧表的容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; // 旧的扩容阈值 int oldThr = threshold; int newCap, newThr = 0; // oldCap大于0说明是对数组进行扩容操作 if (oldCap &gt; 0) &#123; // 如果扩容前旧表的容量大于阈值,就设置为Integer的最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 如果旧表的长度左移一位还小于表的最大容量,就扩容表的长度为旧表的一倍,域值也为原来的一倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold // HashMap的构造方法中会对threshold进行初始化操作 newCap = oldThr; else &#123; // zero initial threshold signifies using defaults // 初始化新容器的大小,必须是2的幂,默认是16。表刚创建的时候,会执行到这里 newCap = DEFAULT_INITIAL_CAPACITY; // 默认的阈值是12 负载因子0.75* 默认的初始化容量16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 新的阈值为0时,按照阈值公式计算阈值 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; // 更新table表扩容时的的阈值 threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) // 用扩容后的容量创建新的数组然后返回数据 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 如果是初始化数组,下面的部分不会执行,下面的部分涉及到数组中元素的移动 // 扩容,进行数据迁移 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 移动旧表的数据到新表中,移动的扩容中,需要重新的进行hash操作 if (e.next == null) // 如果数组中只有一个元素,直接移动即可 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 如果是红黑树,需要对红黑树进行拆分 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order 维持相对顺序 // 链表节点的处理 head是头节点 tail是尾部节点 // loHead是低位链表 hiHead是高位链表 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; // 获取当前节点的下一个节点,每一次循环e值会更新 next = e.next; // 遍历链表,将链表按照计算后的位置进行分组 // 没有再次hash的操作,而是按照扩容后新增加的那个bit是0还是1进行分组 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 分组完成后,将链表映射到桶中 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 扩容后的链表的存储位置 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; // 返回扩容后新表的长度 return newTab; &#125; 代码流程介绍： 1. 判断是初始化或者扩容操作,计算相应的新数组容量newCap 和 新的阈值大小newThr 2. 初始化新的数组容量newCap 3. 如果是扩容操作,还需要进行一个元素移动的操作。在移动的过程中,分为三种操作： 3.1 如果原数组中只有一个元素,并且next指针为空,直接移动即可 newTab[e.hash &amp; (newCap - 1)] = e; 3.2 如果原数组中的元素是树节点。需要将元素进行拆分,然后映射存储位置 3.3 对于链表元素,需要将元素进行拆分。拆分为2个链表。可以理解为低位链表和高位链表,低位链表存储在扩容后的数组原来的位置,高位链表存储在扩容后数组原来的位置+旧数组容量的位置 get方法1234567891011121314151617181920212223242526272829303132333435363738394041/** * 根据key获取其所对应的value值 */ public V get(Object key) &#123; Node&lt;K,V&gt; e; // 计算key值的hashcode。目的是为了通过(n - 1) &amp; hash 计算出当前key值在hash表中的位置 return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; /** * HashMap根据key值和和key的HashCode值查找元素 * * @param hash 当前key的hash值 * @param key 当前key元素 * @return the node, or null if none */ final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; // 如果数组中有元素同时根据hash值计算出当前key值所在的位置不为空 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 检查数组中的元素。判断头结点是否就是需要获取的元素 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 如果头节点中没有,从头结点的下一个节点开始查找 if ((e = first.next) != null) &#123; // 如果头节点是红黑树。则从红黑树中查找 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; // 如果下一个节点不是红黑树。就只能是链表节点。在链表节点中查找元素 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; get方法整体上比较简单,就是根据key的hash值,查找元素在数组的位置,找到之后判断是链表还是红黑树。如果是红黑树,就从红黑树的节点中去寻找。如果是链表节点,就遍历链表去查找相应的key所对应的value的值 put方法put方法相对复杂一些,其实也还好,如果熟悉红黑树这种数据结构的话,看起来也不是很复杂 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * * @param key * @param value * @return */public V put(K key, V value) &#123; // 通过高16位和低16位的异或运算的到一个hash值 return putVal(hash(key), key, value, false, true);&#125;/** * 真正的添加元素方法。尾插法 * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don&#x27;t change existing value 是否进行元素的替换,用于插入相同key的时候,是否替换值,默认值为false,替换相应的值 * @param evict if false, the table is in creation mode. * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // hashmap的table表的初始化操作,是在这里进行的。第一次执行的时候,会先在这里进行初始化操作 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 通过hash值和数组长度进行按位与运算,得到元素的存储位置,如果table表的位置为空,就直接新建一个Node节点进行存储操作 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; // key映射的数组桶位置有元素,也就是产生了hash碰撞(hashmap采用链地址法解决hash冲突。),走下述代码 Node&lt;K,V&gt; e; K k; // 在该位置的第一个数据的key值和插入的元素的key值相等。需要进行下面的if (!onlyIfAbsent || oldValue == null) 的替换操作 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 如果桶中的元素是红黑树节点。就在红黑树中新插入节点,插入完成后,然后调整红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 在链表中插入节点。这里先对链表进行遍历操作 for (int binCount = 0; ; ++binCount) &#123; // 遍历链表时,取下一个位置存放新增的元素,这里采用的是尾插法(链表中不包含要插入键值对节点) // a.横竖都要遍历链表的长度是否大于树化的阈值,所以遍历的过程中,就直接插入元素了 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //如果链表的长度大于8个时,就进行链表转红黑树的操作 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 如果在链表中找到了相同的key值。直接break操作。那么e节点此时就是与链表要插入的新值key相同的Node节点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // key值存在,就替换value值。新插入的元素的value值,替换原来这个key的value值,注意onlyIfAbsent 这个值 // 这个值表示是否仅在oldValue==null 的时候,更新键值对的值。key相同会进行值覆盖 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; // 增加修改次数 ++modCount; // 如果hashmap中元素的值超过了阈值,就会进行扩容操作 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; put方法小结：1.当桶中的元素为空值,通过扩容的方式进行初始化table2.对key进行hash运算。得到需要存储在桶中的位置。如果桶中位置为空,直接在桶中放入元素即可。3.如果桶中的元素不为空。需要先查找插入的key值,是否已经存在,如果已经存在,则需要使用新值替换旧值,4.如果不存在,则将新插入的key值插入在链表的尾部或者红黑树中。其中插入到链表尾部的过程中还会判断链表的长度是否大于树化的阈值。如果大于阈值,就会进行链表转红黑树的操作5.最后判断数组中的数量是否大于扩容阈值,如果大于,会进行数组的扩容操作 转换后的形态,可能如下图所示,我主要为了展示大体的数据结构,不要纠结于元素的值 链表树化JDK1.8 对hashmap进行了改进。采用的数组+链表+红黑树的存储结构。链表转红黑树的操作,主要是为了防止由于hashcode算法性能不佳等原因,造成链表长度过长,查询缓慢的问题,我们元素多的情况下,红黑树是指数级的时间复杂度,在性能上远高于链表的时间复杂度。虽然会有一些额外的空间的消耗,但是时间上能大幅度提升。下面我将介绍 链表树化 、红黑树自平衡、红黑树的基本性质 Node转红黑树节点TreeifyBin()123456789101112131415161718192021222324252627282930313233343536373839404142/** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. 替换指定哈希表的索引桶中所有的连接节点,除非表太小,否则将修改大小 */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; /* * 如果元素数组为空 或者 数组长度小于 树结构化的最小阈值（MIN_TREEIFY_CAPACITY=64） ,就进行扩容操作.对于这个值可以理解为：如果元素数组长度小于这个值,没有必要去进行结构转换.目的是 * 如果数组很小,那么转红黑树,遍历效率要低一些,这时进行扩容操作,重新计算哈希值,链表的长度有可能就变短了。数据会放入到数组中,这样相对来说效率会高一些 */ if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); //如果元素数组长度已经大于等于了 MIN_TREEIFY_CAPACITY,那么就有必要进行结构转换了 // 取出数组对应位置的头节点 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; //定义几个变量,hd(head)代表头节点,tl代表尾节点(tail) 算法中常用变量 TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; //先把普通Node节点转成TreeNode类型,并赋值给p TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); //用新生成的双向链表替代旧的单向链表,其实就是把这个数组对应的位置重新赋值成新双向链表的首节点 hd是index对应的桶的首节点 if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125;// For treeifyBinTreeNode&lt;K,V&gt; replacementTreeNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; return new TreeNode&lt;&gt;(p.hash, p.key, p.value, next);&#125; treeifyBin()方法总结 链表转红黑树的条件： 链表长度大于等于树化阈值 TREEIFY_THRESHOLD = 8 数组长度大于等于MIN_TREEIFY_CAPACITY=64 方法执行步骤： 如果元素数组长度已经大于等于了 MIN_TREEIFY_CAPACITY,就进行结构转换,具体为Node转换为TreeNode节点。否则进行扩容操作 TreeNode节点间接继承自Node节点,所以TreeNode节点包含next引用。原链表顺序最终通过next引用被保存下来 最后一行,调用treeify 将链表转换为红黑树转换后的元素结构,可能是这么一种形态 TreeNode转红黑树treeify()下面就是链表转红黑树的核心逻辑 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 将树形链表转换为红黑树 */final void treeify(Node&lt;K,V&gt;[] tab) &#123; // 定义树的根节点 TreeNode&lt;K,V&gt; root = null; // 遍历链表,x指向当前节点、next指向下一个节点 for (TreeNode&lt;K,V&gt; x = this, next; x != null; x = next) &#123; // 记录x的下一个节点 next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; // 如果根节点为空,则把当前节点当做根节点。根据红黑树性质,根节点一定为黑色。 if (root == null) &#123; x.parent = null; x.red = false; root = x; &#125; // 根节点已经存在的情况 else &#123; // 取得当前遍历的树形节点的 key 和 hash K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; // TODO : 循环开始位置 // 从根节点遍历。这一步主要就是为了判断当前节点应该在树的左侧还是右侧,为节点x找到空位置并插入元素 for (TreeNode&lt;K,V&gt; p = root;;) &#123; // dir代表方向(左边或者右边) ph表示树节点的hash值。 int dir, ph; K pk = p.key; // 比较hash值大小,判断当前节点插入到左边还是右边,并记录dir的值 if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; // 根据比较器判断大小 Comparable 接⼝判断 else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) // 通过key的类名比较 dir = tieBreakOrder(k, pk); // 保存当前遍历的树节点,就是x节点要插入的位置的父节点 TreeNode&lt;K,V&gt; xp = p; // 判断如果dir&lt;=0 说明当前节点的hash值小于当前树节点的hash值。需要把当前节点放置在当前树节点的左侧 // 判断如果dir&gt;0 说明当前节点的hash值大于当前树节点的hash值。 需要把当前节点放置在当前树节点的右侧 // p的左右节点存在不为空的情况,p节点就是当前遍历的树节点,说明该节点还有子节点。继续循环查找当前节点x的应该在哪个爸爸节点下面插入元素 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; // x 节点找到了父节点,并将x节点放入到父节点下面 x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; // 元素插入之后,需要进行红黑树的自平衡操作,重新确定根节点的值 root = balanceInsertion(root, x); break; &#125; &#125; &#125; &#125; // 确保根节点作为第一个节点 moveRootToFront(tab, root);&#125; treeify() 方法执行小结： 遍历第⼀个节点时,此时红⿊树不存在,以第⼀个节点作为红⿊树根节点,并转换为黑节点 有了红⿊树后,此后遍历双链表的每个节点时,都要根据红黑树性质从根节点开始寻找要插⼊当前节点的位置,也就是找到⼀个⽗节点,将当前节点作为其左节点或右节点。 插⼊节点后,可能会导致红⿊树特性被破坏,因此每次插⼊节点后要尝试重新调整红⿊树 红黑树调整完成后,要确保根节点就是table桶中的第一个节点 红黑树的自平衡下面我将介绍红黑树的自平衡操作 这部分代码纯粹就是红黑树的操作了,根hashmap没有多个的关系,有一定数据结构基础的同学,看起来应该没什么难度,对于不了解红黑树的同学,我将在下面代码的下面讲解红黑树的基本操作,帮助你理解如下代码。或者你可以看看我的另一篇文章《红黑树的简单介绍》 * 红黑树插入节点后,需要进行平衡操作 * 情景1： 红黑树为空树时,将根节点染成黑色 * 情景2： 插入的节点在红黑树已经存在,不需要处理 * 情景3： 插入节点的父节点为黑色,因为所插入的路径,黑色节点没有发生变化,所以红黑树依然平衡,所以不需要处理 * 情景4： 插入节点的父节点为红色 * 情景4.1 叔叔节点存在,并且为红色(父和叔 都是红色节点) 根据红黑树性质4.红色节点不能直接相连,由此可知必然存在爷爷节点,且爷爷节点必定为黑节点 * 由此可以 a.将爸爸和叔叔节点变成黑色 b.将爷爷节点变成红色 c.并且将爷爷节点当成当前节点,进行下一轮的处理 * 情景4.2 叔叔节点不存在或者为黑色节点,父节点为爷爷节点的左子树 * 情景4.2.1 插入节点为其父节点的左子节点(LL情况) a.将爸爸节点染成黑色 b.将爷爷染成红色 c.然后以爷爷节点进行右旋操作 * 情景4.2.2 插入节点为其父节点的右子节点(LR情况) a.已爸爸为节点进行一次左旋操作,得到(LL双红的情况 4.2.1) 然后指定爸爸节点为当前节点,执行下一轮的操作 * 情景4.3 叔叔节点不存在,或者为黑色节点,父节点为爷爷节点的右子树 * 情景4.3.1 插入节点为其父节点的右子节点(RR情况) a.将爸爸染成黑色节点 b.将爷爷染成红色 c.然后以爷爷节点进行左旋操作 * 情景4.3.2 插入节点为其父节点的左子节点(RL情况) a.以爸爸节点进行一次右旋,得到RR双红的场景( RR情况 4.3.1).然后指定爸爸节点为当前节点,进行下一轮的操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; // 默认插入节点为红色 x.red = true; // xp为父节点 xpp：为爷爷节点 xxpl是爷爷的左节点(左叔叔节点) xppr：是爷爷节点的右节点(右叔叔节点) for (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) &#123; // 情景1:当前节点的父节点不存在,当前节点就是根节点,根据红黑色性质,根节点为黑色,直接变色就可以了 if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; // 情景2 和情景3：插入节点的父节点为黑色,因为所插入的路径,黑色节点没有发生变化,所以红黑树依然平衡,所以不需要处理。 爷节为空,说明x节点的父节点为根节点,可以直接插入节点 else if (!xp.red || (xpp = xp.parent) == null) return root; // 情景4：如果父节点为红色,同时是父节点是爷爷节点的左节点,这样就遇到了两个红色节点相连的情况,需要进行处理。根据上述公式分为两种情况 if (xp == (xppl = xpp.left)) &#123; // 如果爷爷节点的右节点不为空,同时是红节点(也就是右叔叔节点不为空,且为红色.父叔双红) // 情景4.1 叔叔节点存在,并且为红色(父和叔 都是红色节点) a.将爸爸和叔叔节点变成黑色 b.将爷爷节点变成红色 c.并且将爷爷节点当成当前节点,进行下一轮的处理 // 因为有小伙伴有疑问：这个下一轮处理就是指(xpp节点变成红色后,可能会和xpp节点的父节点发生冲突,也就是两个连续的红色节点,所以需要继续处理) if ((xppr = xpp.right) != null &amp;&amp; xppr.red) &#123; xppr.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; // 情景4.2 叔叔节点不存在或者为黑色节点,父节点为爷爷节点的左子树。分为两种情况 else &#123; // 这些变化操作,你画画图就明白了,单纯的看比较抽象 // 情景4.2.2 插入节点为其父节点的右子节点(LR双红情况) a.以爸爸为节点进行一次左旋操作,得到(LL双红的情况 4.2.1) 然后指定爸爸节点为当前节点,执行下一轮的操作 if (x == xp.right) &#123; root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; // 情景4.2.1 插入节点为其父节点的左子节点(LL情况) a.将爸爸节点染成黑色 b.将爷爷染成红色 c.然后以爷爷节点进行右旋操作 if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateRight(root, xpp); &#125; &#125; &#125; &#125; else &#123; // 情景4.1 叔叔节点存在,并且为红色(父和叔 都是红色节点) 此时爸爸节点为爷爷节点的右节点和上述情况相反 // a.将爸爸和叔叔节点变成黑色 b.将爷爷节点变成红色 c.并且将爷爷节点当成当前节点,进行下一轮的处理(xpp节点变成红色后,可能会和xpp节点的父节点发生冲突,也就是两个连续的红色节点,所以需要继续处理) if (xppl != null &amp;&amp; xppl.red) &#123; xppl.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; // 如果爷爷节点的左节点是黑色或者为空(左叔叔节点) 。那么可能有两种情况： else &#123; // 情景4.3.2 插入节点为其父节点的左子节点(RL情况) a.以爸爸节点进行一次右旋,得到RR双红的场景( RR情况 4.3.1).然后指定爸爸节点为当前节点,进行下一轮的操作 if (x == xp.left) &#123; root = rotateRight(root, x = xp); // 平衡过后,重新定义爷爷节点的变量值 xpp = (xp = x.parent) == null ? null : xp.parent; &#125; // 情景4.3.1 插入节点为其父节点的右子节点(RR情况) a.将爸爸染成黑色节点 b.将爷爷染成红色 c.然后以爷爷节点进行左旋操作 if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateLeft(root, xpp); &#125; &#125; &#125; &#125; &#125;&#125; 纯粹就是红黑树的操作了,这没什么可说的。红黑树需要考虑的场景,我已经在方法上面标注过了,并在代码中给了提示,相信各位同学能很清楚的看明白 红黑树的左旋12345678910111213141516171819202122232425262728293031323334353637383940414243/* ------------------------------------------------------------ */// Red-black tree methods, all adapted from CLR// 红黑树的左旋的过程// 1 将节点 p 旋转为其右节点的左节点,即将节点 p 挂到其右节点的左边// 2 其右节点的左节点成为节点p 的右节点static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateLeft(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; // r -&gt; 节点 p 的右节点 // rl -&gt; 节点 p 的右节点的左节点 // pp -&gt; 节点 p 的⽗节点 TreeNode&lt;K,V&gt; r, pp, rl; // p 不为空且右节点不为空 if (p != null &amp;&amp; (r = p.right) != null) &#123; // 1 将 p 的右节点的左节点挂到 p 的右节点,这有两个信息 // a. 断开 p 与其右节点 r 的连接 // b. 因为 p 要挂到其右节点 r 的左边,因此要把节点 r 原来的左节点挂到 p 的右边 if ((rl = p.right = r.left) != null) // r 节点的左节点的⽗节点重置为 p rl.parent = p; // 2 将 p 的⽗节点设置为 p 的右节点的⽗节点 if ((pp = r.parent = p.parent) == null) // 如果 p 为 root 节点,那么直接将其右节点设置为 root (root = r).red = false; // 3 确定 r 节点应该挂在 p 的⽗节点的左边还是右边。这个根据 p 的位置决定。原来在左边,现在就还在左边 else if (pp.left == p) pp.left = r; else pp.right = r; // 4 将 p 设置为其右节点的左边 r.left = p; // 5 将 p 的右节点指为其⽗节点 p.parent = r; &#125; // 返回根节点 return root;&#125; 单独看上面的左旋方法,可能很抽象,根据节点名称,我画了一个草图帮助你们理解,这个图我已经画的很详细了 红黑树的右旋12345678910111213141516171819202122232425262728293031323334353637383940/*** 红黑树右旋：* 1 将节点 p 旋转为其左节点的右节点,即将节点 p 挂到其左节点的右边* 2 其左节点的右节点成为节点 p 的左节点*/static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateRight(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; // l -&gt; 节点 P 的左节点 // pp -&gt; 节点 p 的⽗节点 // lr -&gt; 节点 p 的左节点的右孩⼦ TreeNode&lt;K,V&gt; l, pp, lr; // 节点 p 和 其左节点不为空 if (p != null &amp;&amp; (l = p.left) != null) &#123; // 1 将 p 的左节点的右孩⼦挂到 p 的左边 if ((lr = p.left = l.right) != null) // 将 p 指定为 lr 的⽗节点 lr.parent = p; // 2 将 p 的⽗节点指定为其右节点的⽗节点 if ((pp = l.parent = p.parent) == null) // 将l节点设置为root节点,并调整为黑色. (root = l).red = false; // 2.1 确定 p 的右节点应该挂在 p的⽗节点左边还是右边.这个根据 p 的位置决定。原来在左边,现在就还在左边 else if (pp.right == p) pp.right = l; else pp.left = l; // 将 p 设置为其左节点的右孩⼦ l.right = p; // 将 p 的⽗节点指定为其左节点 p.parent = l; &#125; // 返回根节点 return root;&#125; 为了帮助理解,同样的,下面增加了红黑树的右旋代码示意图 红黑树的性质 红黑树性质 性质1: 每个节点要么是黑色,要么是红色。 性质2: 根节点是黑色。 性质3: 每个叶子节点(NIL) 是黑色。 性质4: 每个红色节点的两个子节点一定都是黑色。 性质5: 任意节点到每个叶子节 点的路径都包含数量相同的黑结点。从性质5又可以推出: 性质5.1:如果一个节点存在黑子节点, 那么该结点肯定有两个子节点 红黑树的场景插入场景： 情景1：插入的节点为空树直接把插入的节点作为根节点就可以了,并且把根节点变成黑色,如下图所示： 情景2：插入节点的Key已存在处理：更新当前节点的值,为插入节点的值 情景3 插入结点的父结点为黑结点由于插入的结点是红色的,并不会影响红黑树的平衡,直接插入即可,无需做自平衡。如图所示： 情景4：插入节点的父节点为红色红黑树的性质2: 根结点是黑色,如果插入节点的父结点为红结点,那么该父结点不可能为根结点,所以插入结点总是存在祖父结点。 分为两种情况,如下图所示 一种是爸爸节点为红色,叔叔节点也是红色 一种是爸爸节点为红色,叔叔节点为黑色或者不存在 情景4.1叔叔结点存在并且为红结点 依据红黑树性质4 可知,红色节点不能相连==&gt;祖父结点肯定为黑结点。因为不可以同时存在两个相连的红结点。那么此时该插入子树的红黑层数的情况是黑红红。显然最简单的处理方式是把其改为红黑红处理: 1 将爸爸节点(P)和叔叔节点(U)节点改为黑色 2 将爷爷PP改为红色 3 将爷爷PP设置为当前节点,进行后续处理.注意但需要注意的是 PP 变为红⾊后,可能会和它的⽗节点形成连续的红⾊节点,此时需要递归向上调整,也就将 PP 看作新插⼊节点继续尝试调整。 如下图所示： 可以看到,我们把PP结点设为红色了,如果PP的父结点是黑色,那么无需再做任何处理;但如果PP的父结点是红色,则违反了红黑树的性质.所以需要将PP设置为当前节点,继续做插入操作自平衡处理,真到平衡为为止. 插入情景4.2叔叔结点不存在或为黑结点,并且插入结点的父亲结点是祖父结点的左子结点 注意:单纯从插入前来看,叔叔节点非红即空(NIL节点) ,否则的话破坏了红黑树性质5,此路径会比其它路径多一个黑色节点。 新插入节点,可能为P节点的左子节点,也可能是P节点的右子节点,所以分为两种情况分别处理 LL红色情况4.2.1 新插入节点,为其父节点的左子节点处理: 将P设置 为黑色,将PP设置为红色 ,然后以爷爷节点为当前节点 对PP节点进行右旋 处理结果如下图所示： LR红色情况4.2.2 新插入节点,为其父节点的右子节点(LR红色情况)处理: 对P进行左旋 将P设置为当前节点,得到LL红色情况 按照LL红色情况处理(1.变颜色2.右旋PP) 操作过程如下图所示： 第一步 第二步 情况4.3叔叔结点不存在或为黑结点,并且插入结点的父亲结点是祖父结点的右子结点 和上述操作4.2 相反图,如下所示： RR红色情况4.3.1新插入节点,为其父节点的右子节点(RR红色情况) 处理操作： 将P设置为黑色,将PP设置为红色 对PP节点进行左旋 旋转过程如下图所示： RL红色情况4.3.2 新插入节点,为其父节点的左子节点(RL红色情况) 处理:1.对P进行右旋2.将P设置为当前节点,得到RR红色情况3.按照RR红色情况处理(1.变颜色2.左旋PP) 第一步对P节点进行右旋操作 如下图所示： 第二步：变色+旋转,如下图所示 红黑树链化12345678910111213141516171819202122232425 /** * Returns a list of non-TreeNodes replacing those linked from * this node. * 红⿊树中仍然保留了原链表节点顺序。有个这个特点,再将红⿊树转成链表就简单多了,仅需将TreeNode 链表转成 Node 类型的链表即可。 */ final Node&lt;K,V&gt; untreeify(HashMap&lt;K,V&gt; map) &#123; // ⽤于组织链表的头、尾指针 Node&lt;K,V&gt; hd = null, tl = null; // 遍历 TreeNode 链表,并⽤ Node 替换 for (Node&lt;K,V&gt; q = this; q != null; q = q.next) &#123; Node&lt;K,V&gt; p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; &#125; return hd; &#125;// For conversion from TreeNodes to plain nodesNode&lt;K,V&gt; replacementNode(Node&lt;K,V&gt; p, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(p.hash, p.key, p.value, next);&#125; 这段代码链画的过程,相当简单,没什么可说的 红黑树的拆分12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * Splits nodes in a tree bin into lower and upper tree bins, * or untreeifies if now too small. Called only from resize; * see above discussion about split bits and indices. * * @param map the map * @param tab the table for recording bin heads * @param index the index of the table being split * @param bit the bit of hash to split on */final void split(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int index, int bit) &#123; TreeNode&lt;K,V&gt; b = this; // 重新链接到 lo 和 hi 列表,保持顺序 TreeNode&lt;K,V&gt; loHead = null, loTail = null; TreeNode&lt;K,V&gt; hiHead = null, hiTail = null; int lc = 0, hc = 0; /** * 红⿊树节点仍然保留了 next 引⽤,因此仍可以按链表⽅式遍历红⿊树。下⾯的循环是对红⿊树节点进⾏分组,与普通链表操作类似 * 下面这个循环进行的事链表的分组曹组 */ for (TreeNode&lt;K,V&gt; e = b, next; e != null; e = next) &#123; next = (TreeNode&lt;K,V&gt;)e.next; e.next = null; if ((e.hash &amp; bit) == 0) &#123; if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; &#125; else &#123; if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; &#125; &#125; if (loHead != null) &#123; // 如果 loHead 不为空,且链表⻓度⼩于等于 6,则将红⿊树转成链表 if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else &#123; tab[index] = loHead; // hiHead == null 时,表明扩容后,所有节点仍在原位置,树结构不变,⽆需重新树化,否则,将 TreeNode 链表重新树化 if (hiHead != null) // (else is already treeified) loHead.treeify(tab); &#125; &#125; if (hiHead != null) &#123; if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else &#123; tab[index + bit] = hiHead; if (loHead != null) hiHead.treeify(tab); &#125; &#125;&#125; 这段逻辑发生在数据扩容的时候,对于红黑树节点的处理。红黑树的扩容逻辑和链表的扩容逻辑整体上类似。唯一不同的是,除了这段逻辑将红黑树分组后,会判断链表的长度,如果小于UNTREEIFY_THRESHOLD ,会进行红黑树转链表的操作。否则根据条件,将链表树化为红黑树 问题答疑：问题1：问题：为什么集合的初始容量必须是2的n次幂?如果输入值不是2的幂,比如17会怎么样? 这样做是为了减少hash碰撞的次数, 2的n次方实际就是1后面n个0,2的n次方减1二进制表示时实际就是n个1 按位与运算 : 相同的二进制数位上,都是1的时候,结果为1,否则为零。 按位或运算 ： 相同的二进制数位上,都是0的时候,结果为0,否则为1。 12345678910111213141516171819202122232425262728293031323334hash计算存放位置的时候,是通过 hash &amp; (length-1)例如1：hash值如果为3,hashmap默认容量为16 ,即 3 &amp; (16-1)0000 0011 30000 1111 15------------------0000 0011 3 索引值为3例如2：hash值如果为5,hashmap默认容量为16 ,即 5 &amp; (16-1)0000 0101 50000 1111 15------------------0000 0101 5 索引值为5------------------------------------------------如果不是2的n次幂hash值如果为3,hashmap容量设置为17 ,即 3 &amp; (17-1)0000 0011 3 0001 0000 16------------------0000 0000 0 索引位置为0hash值如果为7,hashmap容量设置为17 ,即 7 &amp; (17-1)0000 0111 70001 0000 16------------------0000 0000 0 索引位置为0hash值如果为9,hashmap容量设置为17 ,即 7 &amp; (17-1)0000 1001 90001 0000 16------------------0000 0000 0 索引位置为0 由上面可以看出,当我们根据key的hash确定其在数组的位置时, 如果n为2的幂次方,可以保证数据的均匀插入,如果n不是2的幂次方,可能数组的一些位置永远不会插入数据,浪费数组的空间,加大hash冲突。因此,HashMap 容量为2次幂的原因,就是为了数据的的均匀分布,减少hash冲突,毕竟hash冲突越大, 代表数组中一个链的长度越大,这样的话会降低hashmap的性能 问题2如果初始hashmap的容量不是2的n次幂,会做哪些操作如果创建HashMap对象时,输入的数组长度是17,不是2的幂,HashMap通过移位运算和或运算得到比那个数大且最近的二次幂数字。例如如果容量是17,初始化容量就是返回32 jdk1.8 表改变大小的源码操作如下： 12345678910111213/** * 返回给定目标容量的2次幂大小。 * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; 再次强调一下： 按位与运算 : 相同的二进制数位上,都是1的时候,结果为1,否则为零。 按位或运算 ： 相同的二进制数位上,都是0的时候,结果为0,否则为1。1是负数 0正数 下面看看这几个无符号右移操作是干什么的 第一种情况,容量传递的是0 如果n这时为0了(经过了cap-1之后) , 则经过后面的几次无符号右移依然是0,最后返回的capacity是1 第二种情况,n不等于0 1234567891011121314151617181920212223242526272829303132333435HashMap &lt;String,Object&gt; hashmap = new HashMap&lt;&gt;(17);cap = 17int n = cap - 1 = 16;第一次无符号右移一位0000 0000 0000 0000 0000 0000 0001 0000 n=160000 0000 0000 0000 0000 0000 0000 1000 n &gt;&gt;&gt; 1------------------------------------------------------0000 0000 0000 0000 0000 0000 0001 1000 24 ===&gt;n第二次无符号右移2位0000 0000 0000 0000 0000 0000 0001 1000 n=240000 0000 0000 0000 0000 0000 0000 0110 n &gt;&gt;&gt; 2------------------------------------------------------0000 0000 0000 0000 0000 0000 0001 1110 n = 30第三次无符号右移4位0000 0000 0000 0000 0000 0000 0001 1110 n = 300000 0000 0000 0000 0000 0000 0000 0001 n &gt;&gt;&gt; 4------------------------------------------------------ 0000 0000 0000 0000 0000 0000 0001 1111 n = 31第四次无符号右移8位0000 0000 0000 0000 0000 0000 0001 1111 n=310000 0000 0000 0000 0000 0000 0000 0000 n &gt;&gt;&gt; 8-------------------------------------------------------0000 0000 0000 0000 0000 0000 0001 1111 n=31第五次无符号右移16位0000 0000 0000 0000 0000 0000 0001 1111 n=310000 0000 0000 0000 0000 0000 0000 0000 n &gt;&gt;&gt; 16-------------------------------------------------------0000 0000 0000 0000 0000 0000 0001 1111 n=31执行最后一行代码操作,n&gt;0且小于最大容量,返回31+1 = 32return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; 第三种情况Hashmap最大容量的移动位置操作 123456789101112131415161718190010 0000 0000 0000 0000 0000 0000 0000 2^30int n = cap-1;0001 1111 1111 1111 1111 1111 1111 1111 n=2^30 -1 0000 1111 1111 1111 1111 1111 1111 1111 n &gt;&gt;&gt; 1-------------------------------------------0001 1111 1111 1111 1111 1111 1111 1111 2^290001 1111 1111 1111 1111 1111 1111 1111 2^290000 0111 1111 1111 1111 1111 1111 1111 n&gt;&gt;&gt;2------------------------------------------------0001 1111 1111 1111 1111 1111 1111 1111 2^29.....最后得到的值一定为0001 1111 1111 1111 1111 1111 1111 1111 2^29加一操作后为0010 0000 0000 0000 0000 0000 0000 0000 2^30 总结 ：如果容量大于MAXIMUM_CAPACITY 则取最大容量。不到2^30 的容量,通过移位操作后,会得到大于值的最小2的幂。如果当前值就是2的幂次方,返回当前值 问题3为什么Map桶中的节点个数要超过8才转红黑树因为树节点的大小大约是普通节点的两倍,所以我们只在链表中包含足够的节点时才使用树节点(参见TREEIFY_THRESHOLD)。当它们变得太小(由于删除或调整大小)时,就会被转换回普通的桶。在使用分布良好的用户hashcode时,很少使用树箱。理想情况下,在随机哈希码下,箱子中节点的频率服从泊松分布,默认调整阈值为0.75,平均参数约为0.5,尽管由于调整粒度的差异很大。忽略方差,列表大小k的预期出现次数是(exp(-0.5)*pow(0.5,k)/factorial(k)). * * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * more: less than 1 in ten million1.TreeNodes占用空间是普通Nodes的两倍,所以只有当链表中包含足够多的节点时才会转成TreeNodes,而是否足够多就是由TREEIFY_THRESHOLD（8）的值决定的。当链表中节点数变少时, 红黑树又会转成普通的链表。并且我们查看源码的时候发现,链表长度达到8(桶的数量要大于64)就转成红黑树,当长度降到6就转成普通链表这样就解释了为什么不是一开始就将其转换为TreeNodes, 而是需要一定节点数才转为TreeNodes, 说白了就是权衡,空间和时间的权衡。 2.当hashCode离散性很好的时候,树型节点用到的概率非常小,因为数据均匀分布在每个桶中,几乎不会有桶中链表长度会达到阈值。但是在随机hashCode下,离散性可能会变差,然而JDK又不能阻止用户实现这种不好的hash算法,因此就可能导致不均匀的数据分布。不过理想情况下随机hashCode算法下所有桶中节点的分布频率会遵循泊松分布,我们可以看到,一个桶中链表长度达到8个元素的概率为0.00000006. 几乎是不可能事件。所以,之所以选择8,不是随便决定的,而是根据概率统计决定的。 下面是我找的一个泊松分布 示意图 图片原来链接：http://www.ruanyifeng.com/blog/2015/06/poisson-distribution.html可以参考看下 问题4：基于JDK1.8,hashmap引入了红黑树,为什么一开始不按照红黑树存储。非要等到链表长度大于8才转换 1.JDK 1.8以前HashMap的实现是数组+链表,即使哈希函数取得再好,也很难达到元素百分百均匀分布。当HashMap中有大量的元素都存放到同一个桶中时,这个桶下有一条长长的链表, 这个时候HashMap就相当于一个单链表,假如单链表有n个元素,遍历的时间复杂度就是O(n),完全失去了它的优势。针对这种情况,JDK 1.8中引入了红黑树(查找时间复杂度为O(logn))来优化这个问题。当链表长度很小的时候, 即使遍历,速度也非常快,但是当链表长度不断变长,肯定会对查询性能有一定的影响, 所以才需要转成树。 2.TreeNodes占用空间是普通Nodes的两倍,当元素较少时,增加多余的开销 问题5问题：为什么不使用AVL树而使用红黑树红黑树和AVL树都是最常用的平衡二叉搜索树，它们的查找、删除、修改都是O(lgn) time AVL树和红黑树有几点比较和区别：（1）AVL树是更加严格的平衡，因此可以提供更快的查找速度，一般读取查找密集型任务，适用AVL树。（2）红黑树更适合于插入修改密集型任务。（3）通常，AVL树的旋转比红黑树的旋转更加难以平衡和调试。原文链接：https://blog.csdn.net/21aspnet/article/details/88939297 小结最后提供一个红黑树的学习地址,里面包含的各种数据结构,便于学习和理解数据结构https://www.cs.usfca.edu/~galles/visualization/RedBlack.html","categories":[],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://lswisdom.github.io/tags/Java%E5%9F%BA%E7%A1%80/"}]},{"title":"红黑树的简单介绍","slug":"数据结构/红黑树的简单介绍","date":"2021-08-16T00:07:45.000Z","updated":"2021-10-30T06:53:53.707Z","comments":true,"path":"posts/2388768520/","link":"","permalink":"https://lswisdom.github.io/posts/2388768520/","excerpt":"","text":"红黑树基本讲解1、基本概念 R-B Tree，全称是Red-Black Tree，又称为“红黑树”，它一种特殊的二叉查找树。红黑树的每个节点上都有存储位表示节点的颜色，可以是红(Red)或黑(Black)。 红黑树的特性: 1.1 性质 性质1: 每个节点要么是黑色，要么是红色。 性质2: 根节点是黑色。 性质3: 每个叶子节点(NIL) 是黑色。 性质4: 每个红色节点的两个子节点一定都是黑色。 性质5: 任意节点到每个叶子节 点的路径都包含数量相同的黑结点。从性质5又可以推出:性质5.1:如果一个节点存在黑子节点， 那么该结点肯定有两个子节点 如图所示： 红黑树 并不是一个完美平衡二叉查找树，从图上可以看到，根结点100 的左子树显然比右子树高，但左子树和右子树的黑结点的层数是相等的，也即任意一个结点到到每 个叶子结点的路径都包含数量相同的黑结点(性质5)。所以我们叫红黑树这种平衡为黑色完美平衡。 1.2 自平衡操作 变色 : 结点的颜色由红变黑或由黑变红 **左旋 **: 以某个结点作为支点(旋转结点)，其右子结点变为旋转结点的父结点,右子结点的左子结点变为旋转结点的右子结点，左子结点保持不变 右旋 : 以某个结点作为支点(旋转结点), 其左子结点变为旋转结点的父结点，左子结点的右子结点变为旋转结点的左子结点，右子结点保持不变 红黑树右旋操作如下所示： 红黑树左旋 操作 1.3 红黑树的查找红黑树的查找和二叉树的查找操作类似 1.4 红黑树的插入插入操作包括两部分：1.查找要插入节点的父节点 2.插入后进行树的平衡操作 注意: 插入节点,必须为红色，理由很简单,红色的父节点(如果存在)为黑色节点时，红黑树的黑色平衡没被破坏，不需要做自平衡操作。但如果插入结点是黑色，那么插入位置所在的子树黑色结点总是多1,必须做自平衡。 2.红黑树的插入情景分析情景1：插入的节点为空树直接把插入的节点作为根节点就可以了，并且把根节点变成黑色 情景2：插入节点的Key已存在处理：更新当前节点的值，为插入节点的值 情景3 插入结点的父结点为黑结点由于插入的结点是红色的，并不会影响红黑树的平衡，直接插入即可，无需做自平衡。如图所示： 情景4：插入节点的父节点为红色红黑树的性质2: 根结点是黑色，如果插入节点的父结点为红结点，那么该父结点不可能为根结点，所以插入结点总是存在祖父结点。 分为两种情况，如下图所示 一种是爸爸节点为红色，叔叔节点也是红色 一种是爸爸节点为红色，叔叔节点为黑色或者不存在 情景4.1 叔叔结点存在并且为红结点依据红黑树性质4 可知，红色节点不能相连==&gt;祖父结点肯定为黑结点。因为不可以同时存在两个相连的红结点。那么此时该插入子树的红黑层数的情况是:黑红红。显然最简单的处理方式是把其改为:红黑红处理: 1 将爸爸节点(P)和叔叔节点(U)节点改为黑色 2 将爷爷PP改为红色 3 将爷爷PP设置为当前节点，进行后续处理 如下图所示： 可以看到，我们把PP结点设为红色了，如果PP的父结点是黑色，那么无需再做任何处理;但如果PP的父结点是红色，则违反了红黑树的性质.所以需要将PP设置为当前节点，继续做插入操作自平衡处理，真到平衡为为止. 插入情景4.2 叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的左子结点注意:单纯从插入前来看，叔叔节点非红即空(NIL节点) ，否则的话破坏了红黑树性质5,此路径会比其它路径多-一个黑色节点。 新插入节点，可能为P节点的左子节点，也可能是P节点的右子节点,所以分为两种情况分别处理 4.2.1 新插入节点，为其父节点的左子节点(LL红色情况) 处理: 变颜色:将P设置 为黑色，将PP设置为红色 ，然后以爷爷节点为当前节点 对PP节点进行右旋 处理结果如下图所示： 4.2.2:新插入节点，为其父节点的右子节点(LR红色情况)![](https://ls-picture-oss.oss-cn-hangzhou.aliyuncs.com/数据结构/红黑树/4.2.2 LR情况处理前.png?versionId=CAEQGxiBgMDGgpax2hciIDQ5OWRlYzEyMDYzMzQzYTBiZGU4YzJiNDdjMzRiOTNh) 处理: 对P进行左旋 将P设置为当前节点，得到LL红色情况 按照LL红色情况处理(1.变颜色2.右旋PP) 操作过程如下图所示： 第一步 ![](https://ls-picture-oss.oss-cn-hangzhou.aliyuncs.com/数据结构/红黑树/4.2.2 LR情况处理第一步.png?versionId=CAEQGxiBgMDLgpax2hciIDAyZmQ4NzY0NjBjNTQ4MTJiMmUxNDIyZTViZWI1MzBk) 第二步 情况4.3 叔叔结点不存在或为黑结点，并且插入结点的父亲结点是祖父结点的右子结点和上述操作4.2 相反图，如下所示： 4.3.1 新插入节点，为其父节点的右子节点(RR红色情况) 处理操作：1.变颜色: 将P设置为黑色，将PP设置为红色 2.对PP节点进行左旋 旋转过程如下图所示： 4.3.2 新插入节点，为其父节点的左子节点(RL红色情况)如下图所示： ![](https://ls-picture-oss.oss-cn-hangzhou.aliyuncs.com/数据结构/红黑树/4.3.2 RL情况.png?versionId=CAEQGxiBgMClhpax2hciIDc1Yjg2NjViMjIyMjQ4Mzk4MTUyNTEyOWQ2NDA2NGUw) 处理:1.对P进行右旋2.将P设置为当前节点，得到RR红色情况3.按照RR红色情况处理(1.变颜色2.左旋PP) 第一步对P节点进行右旋操作 如下图所示： 第二步：变色+旋转，如下图所示 3.红黑树代码演示3.1 左右旋参考示意图 3.2 代码演示RBTree.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380/** * 红黑树 */public class RBTree&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private static final boolean RED = true; private static final boolean BLACK = false; private RBNode root; public RBNode getRoot() &#123; return root; &#125; /** * 获取当前节点的父节点 * * @param node * @return */ private RBNode parentOf(RBNode node) &#123; if (node != null) &#123; return node.parent; &#125; return null; &#125; /** * 节点是否为红色 * * @param node * @return */ private boolean isRed(RBNode node) &#123; if (node != null) &#123; return node.color == RED; &#125; return false; &#125; /** * 节点是否为黑色 * * @param node * @return */ private boolean isBlack(RBNode node) &#123; if (node != null) &#123; return node.color == BLACK; &#125; return false; &#125; /** * 设置节点为红色 * * @param node */ private void setRed(RBNode node) &#123; if (node != null) &#123; node.color = RED; &#125; &#125; /** * 设置节点颜色为黑色 * * @param node */ private void setBlack(RBNode node) &#123; if (node != null) &#123; node.color = BLACK; &#125; &#125; public void inOrderPrint() &#123; inOrderPrint(this.root); &#125; private void inOrderPrint(RBNode node) &#123; if (node != null) &#123; inOrderPrint(node.left); System.out.println(&quot; key: &quot; + node.key + &quot; ,value:&quot; + node.value); inOrderPrint(node.right); &#125; &#125; /** * 左旋示意图 */ private void leftRotate(RBNode x) &#123; RBNode y = x.right; x.right = y.left; // 1.将x的右子节点指定y的左子节点（ly) ,将y的左子节点的父节点更新为x if (y.left != null) &#123; y.left.parent = x; &#125; // 2.当x的父节点(不为空时) ,更新y的父节点为x的父节点 ，并将x的父节点的 子树（当前x的指数位置）指定为y if (x.parent != null) &#123; // 如果x节点原来为其父节点的左节点，左旋时，就需要把y节点也放在左节点的位置 if (x == x.parent.left) &#123; x.parent.left = y; &#125; else &#123; // 如果x节点原来为其父节点的右节点，左旋时，就需要把y节点也放在左节点的位置 x.parent.right = y; &#125; &#125; else &#123; // 说民x节点为根节点 故所以更新y节点为根节点 this.root = y; this.root.parent = null; &#125; // 3.将x的父节点更新为y ,并将y的左子节点更新为x x.parent = y; y.left = x; &#125; /** * 右旋操作 * * * @param y */ private void rightRotate(RBNode y) &#123; RBNode x = y.left; // 1.将y的左子节点更新为x的右子节点，并且更新x的右子节点的父节点为y y.left = x.right; if (x.right != null) &#123; x.right.parent = y; &#125; // 2.当y的父节点不为空时，更新x的父节点为y的父节点。更新y的父节点的指定子节点为x if (y.parent != null) &#123; x.parent = y.parent; // 此时不知道原先y是在其父节点的左侧还是右侧，所以要先判断 if (y == y.parent.left) &#123; y.parent.left = x; &#125; else &#123; y.parent.right = x; &#125; &#125; else &#123; // 没有父节点 this.root = x; this.root.parent = null; &#125; // 3.更新y的节点为x,并且更新x的节点为y y.parent = x; x.right = y; &#125; /** * 插入方法 * * @param key 插入key * @param value 值 */ public void insertNode(K key, V value) &#123; RBNode rbNode = new RBNode(); rbNode.setKey(key); rbNode.setValue(value); rbNode.setColor(RED); insert(rbNode); &#125; /** * 插入红黑树节点 * * @param node 节点 */ private void insert(RBNode node) &#123; // 1.查找当前节点的父节点 RBNode parent = null; // 获取根节点，开始寻找 RBNode x = this.root; while (x != null) &#123; parent = x; int result = node.key.compareTo(x.key); if (result &gt; 0) &#123; // 说明当前节点应该放在x节点的右子树 x = x.right; &#125; else if (result &lt; 0) &#123; // 说明当前节点应该放在左子树上 x = x.left; &#125; else &#123; // 如果key值相等，需要进行值的替换 x.setValue(node.getValue()); return; &#125; &#125; // 记录父节点 node.parent = parent; if (parent != null) &#123; // 判断当前节点和父节点的key值大小，从而确定新插入的节点应该放在父节点的左侧还是右侧 int result = node.key.compareTo(parent.key); if (result &gt; 0) &#123; parent.right = node; &#125; else &#123; parent.left = node; &#125; &#125; else &#123; // 如果没有父亲节点,当前节点就是根节点 this.root = node; &#125; // 红黑树插入新节点后，需要进行平衡操作，也就是左旋或者右旋操作 balanceInsertion(node); &#125; /** * 红黑树插入节点后，需要进行平衡操作 * 情景1： 红黑树为空树时,将根节点染成黑色 * 情景2： 插入的节点在红黑树已经存在,不需要处理 * 情景3： 插入节点的父节点为黑色，因为所插入的路径，黑色节点没有发生变化，所以红黑树依然平衡，所以不需要处理 * 情景4： 插入节点的父节点为红色 * 情景4.1 叔叔节点存在,并且为红色(父和叔 都是红色节点) 根据红黑树性质4.红色节点不能直接相连，由此可知必然存在爷爷节点,且爷爷节点必定为黑节点 * 由此可以 a.将爸爸和叔叔节点变成黑色 b.将爷爷节点变成红色 c.并且将爷爷节点当成当前节点,进行下一轮的处理 * 情景4.2 叔叔节点不存在或者为黑色节点，父节点为爷爷节点的左子树 * 情景4.2.1 插入节点为其父节点的左子节点(LL情况) a.将爸爸节点染成黑色 b.将爷爷染成红色 c.然后已爷爷节点进行右旋操作 * 情景4.2.2 插入节点为其父节点的右子节点(LR情况) a.已爸爸为节点进行一次左旋操作,得到(LL双红的情况 4.2.1) 然后指定爸爸节点为当前节点,执行下一轮的操作 * 情景4.3 叔叔节点不存在,或者为黑色节点,父节点为爷爷节点的右子树 * 情景4.3.1 插入节点为其父节点的右子节点(RR情况) a.将爸爸染成黑色节点 b.将爷爷染成红色 c.然后已爷爷节点进行左旋操作 * 情景4.3.2 插入节点为其父节点的左子节点(RL情况) a.以爸爸节点进行一次右旋,得到RR双红的场景( RR情况 4.3.1).然后指定爸爸节点为当前节点,进行下一轮的操作 * @param node */ private void balanceInsertion(RBNode node) &#123; this.root.setColor(BLACK); // 1.获取父节点和爷爷节点 RBNode parent = parentOf(node); RBNode gparent = parentOf(parent); // 情景3 : 插入节点的父节点为黑色,不需要处理 // 情景4： 插入节点的父节点为红色 if (parent != null &amp;&amp; isRed(parent)) &#123; // 叔叔节点 RBNode uncle = null; // 如果爸爸是爷爷的左孩子，那么叔叔必然是爷爷的右孩子,反之一样 if (parent == gparent.left) &#123; uncle = gparent.right; // 情景4.1 叔叔节点存在,并且为红色(父和叔 都是红色节点) if (uncle != null &amp;&amp; isRed(uncle)) &#123; // a.将爸爸和叔叔节点变成黑色 b.将爷爷节点变成红色 c.并且将爷爷节点当成当前节点,进行下一轮的处理 setBlack(parent); setBlack(uncle); setRed(gparent); balanceInsertion(gparent); return; &#125; // 情景4.2 叔叔节点不存在或者为黑色节点，父节点为爷爷节点的左子树 if (uncle == null || isBlack(uncle)) &#123; // 情景4.2.1 插入节点为其父节点的左子节点(LL情况) if (node == parent.left) &#123; // a.将爸爸节点染成黑色 b.将爷爷染成红色 c.然后已爷爷节点进行右旋操作 setBlack(parent); setRed(gparent); rightRotate(gparent); return; &#125; // 情景4.2.2 插入节点为其父节点的右子节点(LR情况) if (node == parent.right) &#123; // a.已爸爸为节点进行一次左旋操作,得到(LL双红的情况 4.2.1) 然后指定爸爸节点为当前节点,执行下一轮的操作 leftRotate(parent); balanceInsertion(parent); return; &#125; &#125; &#125; else &#123; // 父节点为爷爷节点的右子树,则叔叔节点就是爷爷节点的左子树 uncle = gparent.left; // 情景4.1 叔叔节点存在,并且为红色(父和叔 都是红色节点) if (uncle != null &amp;&amp; isRed(uncle)) &#123; // a.将爸爸和叔叔节点变成黑色 b.将爷爷节点变成红色 c.并且将爷爷节点当成当前节点,进行下一轮的处理 setBlack(parent); setBlack(uncle); setRed(gparent); balanceInsertion(gparent); return; &#125; // 情景4.3 叔叔节点不存在,或者为黑色节点,父节点为爷爷节点的右子树 if (uncle == null || isBlack(uncle)) &#123; // 情景4.3.1 插入节点为其父节点的右子节点(RR情况) a.将爸爸染成黑色节点 b.将爷爷染成红色 c.然后已爷爷节点进行左旋操作 if (node == parent.right) &#123; setBlack(parent); setRed(gparent); leftRotate(gparent); return; &#125; // 情景4.3.2 插入节点为其父节点的左子节点(RL情况) if (node == parent.left) &#123; // a.以爸爸节点进行一次右旋,得到RR双红的场景( RR情况 4.3.1).然后指定爸爸节点为当前节点,进行下一轮的操作 rightRotate(parent); balanceInsertion(parent); return; &#125; &#125; &#125; &#125; &#125; static class RBNode&lt;K extends Comparable&lt;K&gt;, V&gt; &#123; private RBNode parent; private RBNode left; private RBNode right; private boolean color; private K key; private V value; public RBNode() &#123; &#125; public RBNode(RBNode parent, RBNode left, RBNode right, boolean color, K key, V value) &#123; this.parent = parent; this.left = left; this.right = right; this.color = color; this.key = key; this.value = value; &#125; public RBNode getParent() &#123; return parent; &#125; public void setParent(RBNode parent) &#123; this.parent = parent; &#125; public RBNode getLeft() &#123; return left; &#125; public void setLeft(RBNode left) &#123; this.left = left; &#125; public RBNode getRight() &#123; return right; &#125; public void setRight(RBNode right) &#123; this.right = right; &#125; public boolean isColor() &#123; return color; &#125; public void setColor(boolean color) &#123; this.color = color; &#125; public K getKey() &#123; return key; &#125; public void setKey(K key) &#123; this.key = key; &#125; public V getValue() &#123; return value; &#125; public void setValue(V value) &#123; this.value = value; &#125; &#125;&#125; 3.3 运行结果演示 没做类型处理，插入的都是字符串比较，按照字符串的ASICC进行排序和平衡，颜色就是区分显示，不表示红黑色 4.红黑树学习地址1https:&#x2F;&#x2F;www.cs.usfca.edu&#x2F;~galles&#x2F;visualization&#x2F;RedBlack.html","categories":[],"tags":[{"name":"数据结构","slug":"数据结构","permalink":"https://lswisdom.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"}]},{"title":"JDK1.8 HashMap源码简单讲解","slug":"Java基础/JDK1.8 HashMap源码讲解","date":"2021-08-16T00:00:00.000Z","updated":"2021-08-16T00:19:44.975Z","comments":true,"path":"posts/221454775/","link":"","permalink":"https://lswisdom.github.io/posts/221454775/","excerpt":"","text":"JDK1.8 HashMap源码简单讲解1.成员常量类1.1 序列化版本号1private static final long serialVersionUID = 362498820763181265L; 1.2 集合的初始容量12345/** * The default initial capacity - MUST be a power of two. * 默认容量是16,必须是2的n的n次幂 */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 1.2.1 问题：为什么必须是2的n次幂?如果输入值不是2的幂,比如17会怎么样?这样做是为了减少hash碰撞的次数， 2的n次方实际就是1后面n个0，2的n次方减1二进制表示时实际就是n个1; 按位与运算 : 相同的二进制数位上,都是1的时候，结果为1,否则为零。 按位或运算 ： 相同的二进制数位上,都是0的时候，结果为0,否则为1。 12345678910111213141516171819202122232425262728293031323334hash计算存放位置的时候,是通过hash&amp;(length-1)例如1：hash值如果为3,hashmap默认容量为16 ,即 3 &amp; (16-1)0000 0011 30000 1111 15------------------0000 0011 3 索引值为3例如2：hash值如果为5,hashmap默认容量为16 ,即 5 &amp; (16-1)0000 0101 50000 1111 15------------------0000 0101 5 索引值为5------------------------------------------------如果不是2的n次幂hash值如果为3,hashmap容量设置为17 ,即 3 &amp; (17-1)0000 0011 3 0001 0000 16------------------0000 0000 0 索引位置为0hash值如果为7,hashmap容量设置为17 ,即 7 &amp; (17-1)0000 0111 70001 0000 16------------------0000 0000 0 索引位置为0hash值如果为9,hashmap容量设置为17 ,即 7 &amp; (17-1)0000 1001 90001 0000 16------------------0000 0000 0 索引位置为0 总结： ​ 1.由上面可以看出，当我们根据key的hash确定其在数组的位置时， 如果n为2的幂次方，可以保证数据的均匀插入，如果n不是2的幂次方，可能数组的一些位置永远不会插入数据，浪费数组的空间，加大hash冲突。因此，HashMap 容量为2次幂的原因，就是为了数据的的均匀分布，减少hash冲突,毕竟hash冲突越大, 代表数组中一个链的长度越大，这样的话会降低hashmap的性能 1.2.2 为什么不直接进行取余运算，而是通过位运算在计算中，数据是采用0101存储的，位运算的效率远比取模%高，所以，使用位运算代替取余操作，来确定元素的存储问题。当前两种方式，得到的结果都是一样的，只是效率不同 1hash &amp; (length-1)&#x3D;&#x3D; hash % length 1.2.3 如果初始hashmap的容量不是2的n次幂，会做哪些操作如果创建HashMap对象时，输入的数组长度是17,不是2的幂，HashMap通过移位运算和或运算得到的肯定是2的幂次数,并且是离那个数最近的数字。例如如果容量是17，初始化容量就是返回32 下述源码就是初始化时指定大小和负载因子 jdk1.8源码操作如下： 123456789101112131415161718192021222324252627282930313233343536373839 /** * Constructs an empty &lt;tt&gt;HashMap&lt;/tt&gt; with the specified initial * capacity and load factor. * * @param initialCapacity the initial capacity * @param loadFactor the load factor * @throws IllegalArgumentException if the initial capacity is negative * or the load factor is nonpositive */ public HashMap(int initialCapacity, float loadFactor) &#123; // 如果初始容量小于0,抛出异常 if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); // 如果初始容量大于最大容量2^30,复制为最大容量。防止溢出 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; // 负载因子小于0,报错提示 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; // threshold hashmap扩容阈值,注意这个值会发生变化。 this.threshold = tableSizeFor(initialCapacity); &#125; /** * 返回给定目标容量的2次幂大小。 * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; 上述代码第二个方法，就是操作 说明: 由此可以看到，当在实例化HashMap实例时， 如果给定了initialCapacity(假设是17),由于HashMap的capacity必须都是2的幂次方，因此这个方法用于找到大于等于initialCapacity(假设是17)的最小的2的幂(initialCapacity如果就是2的幂次方，比如说16，则返回的还是这个数)。 下面分析这段源码算法: a.首先,为什么要对cap做减1操作1int n &#x3D; cap - 1 如果cap已经是2的幂，此时又没有执行这个减1操作, 则执行完后面的几条无符号右移操作之后，返回的capacity将是这个cap的2倍。 注意： 按位与运算 : 相同的二进制数位上,都是1的时候，结果为1,否则为零。 按位或运算 ： 相同的二进制数位上,都是0的时候，结果为0,否则为1。 b.下面看看这几个无符号右移操作是干什么的 1是负数 0正数 ​ 第一种情况，容量传递的是0 ​ 如果n这时为0了(经过了cap-1之后) , 则经过后面的几次无符号右移依然是0,最后返回的capacity是1 ​ **第二种情况，n不等于0 ** 12345678910111213141516171819202122232425262728293031323334HashMap &lt;String,Object&gt; hashmap = new HashMap&lt;&gt;(17);cap = 17int n = cap - 1 = 16;第一次无符号右移一位0000 0000 0000 0000 0000 0000 0001 0000 n=160000 0000 0000 0000 0000 0000 0000 1000 n &gt;&gt;&gt; 1------------------------------------------------------0000 0000 0000 0000 0000 0000 0001 1000 24 ===&gt;n第二次无符号右移2位0000 0000 0000 0000 0000 0000 0001 1000 n=240000 0000 0000 0000 0000 0000 0000 0110 n &gt;&gt;&gt; 2------------------------------------------------------0000 0000 0000 0000 0000 0000 0001 1110 n = 30第三次无符号右移4位0000 0000 0000 0000 0000 0000 0001 1110 n = 300000 0000 0000 0000 0000 0000 0000 0001 n &gt;&gt;&gt; 4------------------------------------------------------ 0000 0000 0000 0000 0000 0000 0001 1111 n = 31第四次无符号右移8位0000 0000 0000 0000 0000 0000 0001 1111 n=310000 0000 0000 0000 0000 0000 0000 0000 n &gt;&gt;&gt; 8-------------------------------------------------------0000 0000 0000 0000 0000 0000 0001 1111 n=31第五次无符号右移16位0000 0000 0000 0000 0000 0000 0001 1111 n=310000 0000 0000 0000 0000 0000 0000 0000 n &gt;&gt;&gt; 16-------------------------------------------------------0000 0000 0000 0000 0000 0000 0001 1111 n=31执行最后一行代码操作,n&gt;0且小于最大容量,返回31+1 = 32return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; Hashmap最大容量的移动位置操作 123456789101112131415161718190010 0000 0000 0000 0000 0000 0000 0000 2^30int n = cap-1;0001 1111 1111 1111 1111 1111 1111 1111 n=2^30 -1 0000 1111 1111 1111 1111 1111 1111 1111 n &gt;&gt;&gt; 1-------------------------------------------0001 1111 1111 1111 1111 1111 1111 1111 2^290001 1111 1111 1111 1111 1111 1111 1111 2^290000 0111 1111 1111 1111 1111 1111 1111 n&gt;&gt;&gt;2------------------------------------------------0001 1111 1111 1111 1111 1111 1111 1111 2^29.....最后得到的值一定为0001 1111 1111 1111 1111 1111 1111 1111 2^29加一操作后为0010 0000 0000 0000 0000 0000 0000 0000 2^30 ** 总结** ：如果容量大于MAXIMUM_CAPACITY 则取最大容量。不到2^30 的容量，通过移位操作后，会得到大于值的最小2的幂。如果当前值就是2的幂次方，返回当前值 1.3 负载因子12345/** * 负载因子 * The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f; 对于负载因子，如果没有特别需求，不要轻易进行更改，因为 JDK 自身的默认负载因子是非常符合通用场景的需求的（逼哥讲过大量研究得出）。如果确实需要调整，建议不要设置超过 0.75 的数值，因为会显著增加冲突，降低 HashMap 的性能。如果使用太小的负载因子，可能会导致更加频繁的扩容，增加无谓的开销，本身访问性能也会受影响。 loadFactor越趋近于1,那么数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加。 loadFactor越小，也就是趋近于0，数组中存放的数据(entry)也就越少，也就越稀疏。 如果希望链表尽可能少些。要提前扩容，有的数组空间有可能一-直没有存储数据。加载因子尽可能小一些。举例:1.加载因子是0.5。那么16 * 0.5 = 8 如果数组中满8个空间就扩容，这样会造成数组利用率太低了。2.加载因子是0.9。那么16 * 0.9=14 如果数组中满14个空间就扩容，hash碰撞次数大大增加，那么这样就会导致链表有点多了。导致查找元素效率低。 所以既兼顾数组利用率又考虑链表长度不要太多，经过逼哥 大量测试0.75是最佳方案。 1.4 链表转红黑树的阈值8123456789/** * The bin count threshold for using a tree rather than list for a * bin. Bins are converted to trees when adding an element to a * bin with at least this many nodes. The value must be greater * than 2 and should be at least 8 to mesh with assumptions in * tree removal about conversion back to plain bins upon * shrinkage. */static final int TREEIFY_THRESHOLD = 8; 问题1： 为什么Map桶中的节点个数要超过8才转红黑树源码注释中有这么一段描述 12345678910111213141516171819202122232425262728293031323334 * Because TreeNodes are about twice the size of regular nodes, we * use them only when bins contain enough nodes to warrant use * (see TREEIFY_THRESHOLD). And when they become too small (due to * removal or resizing) they are converted back to plain bins. In * usages with well-distributed user hashCodes, tree bins are * rarely used. Ideally, under random hashCodes, the frequency of * nodes in bins follows a Poisson distribution * (http://en.wikipedia.org/wiki/Poisson_distribution) with a * parameter of about 0.5 on average for the default resizing * threshold of 0.75, although with a large variance because of * resizing granularity. Ignoring variance, the expected * occurrences of list size k are (exp(-0.5) * pow(0.5, k) / * factorial(k)). The first values are:因为树节点的大小大约是普通节点的两倍，所以我们只在bin包含足够的节点时才使用树节点(参见TREEIFY_THRESHOLD)。当它们变得太小(由于删除或调整大小)时，就会被转换回普通的桶。在使用分布良好的用户hashcode时，很少使用树箱。理想情况下，在随机哈希码下，箱子中节点的频率服从泊松分布(http://en.wikipedia. org/wiki/Poisson. distr ibution),默认调整阈值为0.75,平均参数约为0.5,尽管由于调整粒度的差异很大。忽略方差，列表大小k的预期出现次数是(exp(-0.5)*pow(0.5，k)/factorial(k)). * * 0: 0.60653066 * 1: 0.30326533 * 2: 0.07581633 * 3: 0.01263606 * 4: 0.00157952 * 5: 0.00015795 * 6: 0.00001316 * 7: 0.00000094 * 8: 0.00000006 * more: less than 1 in ten million * * The root of a tree bin is normally its first node. However, * sometimes (currently only upon Iterator.remove), the root might * be elsewhere, but can be recovered following parent links * (method TreeNode.root()). 说明： 1.TreeNodes占用空间是普通Nodes的两倍，所以只有当bin包含足够多的节点时才会转成TreeNodes,而是否足够多就是由TREEIFY_THRESHOLD（8）的值决定的。当bin中节点数变少时， 又会转成普通的bin。并且我们查看源码的时候发现，链表长度达到8(桶的数量要大于64)就转成红黑树，当长度降到6就转成普通bin.这样就解释了为什么不是一开始就将其转换为TreeNodes, 而是需要一定节点数才转为TreeNodes, 说白了就是权衡，空间和时间的权衡。 2.当hashCode离散性很好的时候，树型bin用到的概率非常小，因为数据均匀分布在每个bin中，几乎不会有bin中链表长度会达到阈值。但是在随机hashCode下，离散性可能会变差，然而JDK又不能阻止用户实现这种不好的hash算法，因此就可能导致不均匀的数据分布。不过理想情况下随机hashCode算法下所有bin中节点的分布频率会遵循泊松分布，我们可以看到，一个bin中链表长度达到8个元素的概率为0.00000006. 几乎是不可能事件。所以，之所以选择8，不是随便决定的，而是根据概率统计决定的。 也就是说:选择8因为符合泊松分布，超过8的时候，概率已经非常小了，所以我们选择8这个数字。 3.泊松分布 图片原文链接： http://www.ruanyifeng.com/blog/2015/06/poisson-distribution.html 问题2：基于JDK1.8,hashmap引入了红黑树，为什么一开始不按照红黑树存储。要等到链表长度大于8才转换​ JDK 1.8以前HashMap的实现是数组+链表，即使哈希函数取得再好，也很难达到元素百分百均匀分布。当HashMap中有大量的元素都存放到同一个桶中时，这个桶下有一条长长的链表， 这个时候HashMap就相当于一个单链表，假如单链表有n个元素,遍历的时间复杂度就是O(n),完全失去了它的优势。针对这种情况，JDK 1.8中引入了红黑树(查找时间复杂度为O(logn))来优化这个问题。当链表长度很小的时候， 即使遍历,速度也非常快,但是当链表长度不断变长，肯定会对查询性能有一定的影响， 所以才需要转成树。 2.TreeNodes占用空间是普通Nodes的两倍，当元素较少时，增加多余的开销 1.5 红黑树退化为链表的阈值1234567/** * The bin count threshold for untreeifying a (split) bin during a * resize operation. Should be less than TREEIFY_THRESHOLD, and at * most 6 to mesh with shrinkage detection under removal. 当桶(bucket) 上的结点数小于这个值时树转链表 */ static final int UNTREEIFY_THRESHOLD = 6; 1.6 链表转红黑树时，数组的长度最小值1234567/** * The smallest table capacity for which bins may be treeified. * (Otherwise the table is resized if too many nodes in a bin.) * Should be at least 4 * TREEIFY_THRESHOLD to avoid conflicts * between resizing and treeification thresholds. */ static final int MIN_TREEIFY_CAPACITY = 64; 1.7 table 用来初始化table必须是2的幂 1234567/** * The table, initialized on first use, and resized as * necessary. When allocated, length is always a power of two. * (We also tolerate length zero in some operations to allow * bootstrapping mechanics that are currently not needed.) */transient Node&lt;K,V&gt;[] table; jdk8之前数组类型是Entry&lt;K,V&gt;类型。从jdk1.8之后是Node&lt;K,V&gt;类型。 都实现了一样的接口:Map.Entry&lt;K,V&gt;.负责存储键值对数据的。在jdk8以后的构造方法中，并没有对table这个成员变量进行初始化，tabl e的初始化被推迟到了put方法中 1.8 存放元素缓存12345/** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */ transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; 1.9 HashMap元素的个数1234/** * The number of key-value mappings contained in this map. */transient int size; 1.10 用来记录HashMap的修改次数123456789/** * The number of times this HashMap has been structurally modified * Structural modifications are those that change the number of mappings in * the HashMap or otherwise modify its internal structure (e.g., * rehash). This field is used to make iterators on Collection-views of * the HashMap fail-fast. (See ConcurrentModificationException). 结构修改是指改变HashMap中的映射数量或修改其内部结构(例如，rehash) */transient int modCount; 1.11 要调整大小的下一个大小值（容量*负载系数）12345678910/** * The next size value at which to resize (capacity * load factor). * 要调整大小的下一个大小值（容量*负载系数）。 数组长度唱过临界值时会进行扩容操作 * @serial */// (The javadoc description is true upon serialization.// Additionally, if the table array has not been allocated, this// field holds the initial array capacity, or zero signifying// DEFAULT_INITIAL_CAPACITY.)int threshold; 1.12 解决hash冲突的常用方法有 1.开放定址法基本思想是：当关键字key的哈希地址p=H（key）出现冲突时，以p为基础，产生另一个哈希地址p1，如果p1仍然冲突，再以p为基础，产生另一个哈希地址p2，…，直到找出一个不冲突的哈希地址pi ，将相应元素存入其中。 2.再哈希法这种方法是同时构造多个不同的哈希函数：Hi=RH1（key） i=1，2，…，k当哈希地址Hi=RH1（key）发生冲突时，再计算Hi=RH2（key）……，直到冲突不再产生。这种方法不易产生聚集，但增加了计算时间。 3.链地址法这种方法的基本思想是将所有哈希地址为i的元素构成一个称为同义词链的单链表，并将单链表的头指针存在哈希表的第i个单元中，因而查找、插入和删除主要在同义词链中进行。链地址法适用于经常进行插入和删除的情况。 hashmap解决hash冲突就是采用链地址法 4.建立公共溢出区这种方法的基本思想是：将哈希表分为基本表和溢出表两部分，凡是和基本表发生冲突的元素，一律填入溢出表。 2.hashmap的方法介绍Node节点 1234567891011121314151617/** * Basic hash bin node, used for most entries. (See below for * TreeNode subclass, and in LinkedHashMap for its Entry subclass.) */ static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; TreeNode节点部分源码如下： 12345678910111213141516171819202122232425/** * Entry for Tree bins. Extends LinkedHashMap.Entry (which in turn * extends Node) so can be used as extension of either regular or * linked node. */ static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; /** * Returns root of tree containing this node. */ final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; &#125; 2.1 hash值计算方法 hash()jdk1.8 源码如下： 12345678910111213141516171819202122/** * 计算hash值的方法 * Computes key.hashCode() and spreads (XORs) higher bits of hash * to lower. Because the table uses power-of-two masking, sets of * hashes that vary only in bits above the current mask will * always collide. (Among known examples are sets of Float keys * holding consecutive whole numbers in small tables.) So we * apply a transform that spreads the impact of higher bits * downward. There is a tradeoff between speed, utility, and * quality of bit-spreading. Because many common sets of hashes * are already reasonably distributed (so don&#x27;t benefit from * spreading), and because we use trees to handle large sets of * collisions in bins, we just XOR some shifted bits in the * cheapest possible way to reduce systematic lossage, as well as * to incorporate impact of the highest bits that would otherwise * never be used in index calculations because of table bounds. 计算 key.hashCode() 并将散列的较高位（异或）传播到较低位。由于该表使用二次幂掩码，因此仅在当前 掩码之上位变化的散列集将始终发生冲突。 （众所周知的例子是在小表中保存连续整数的浮点键集。）所以我们应用一个变换，将高位的影响向下传播。位扩展的速度、效用和质量之间存在折衷。因为许多常见的散列集已经合理分布（因此不会从扩展中受益），并且因为我们使用树来处理 bin 中的大量冲突，所以我们只是以最便宜的方式对一些移位的位进行异或以减少系统损失，以及合并最高位的影响，否则由于表边界而永远不会在索引计算中使用。 */static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 我们先研究下key的哈希值是如何计算出来的。key的哈希值是通过上述方法计算出来的。 可以看到当key等于null的时候也是有哈希值的，返回的是0. 这个哈希方法首先计算出key的hashCode赋值给h,然后与h无符号右移16位后的二进制进行按位异或得到最后的hash值。计算过程如下所示: 1(h &#x3D; key.hashCode()) ^ (h &gt;&gt;&gt; 16); 这里其实就是把得到的hashcode转化为32位二进制，然后他的高16位和低16位做了一个异或的操作 举个例子如下： 12345678910111213141516171819202122232425&amp; (按位与运算) : 运算规则:相同的二进制数位上,都是1的时候，结果为1,否则为零。^ (按位异或运算) :运算规则:相同的二进制数位上，数字相同，结果为0，不同为1.代码中通过这个hash &amp; (n-1) 得到存储元素的位置p = tab[i = (n - 1) &amp; hash](h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);假设这里的h = key.hashCode()得到的值为1111 1111 1111 1111 0000 0000 1111 1111 h0000 0000 0000 0000 1111 1111 1111 1111 h &gt;&gt;&gt; 16----------------------------------------------------------1111 1111 1111 1111 1111 1111 0000 0000 按位异或运算后得到的hash值此时数据长度n假设为默认的16,那么这个key存放在table中位置为i = (n - 1) &amp; hash1111 1111 1111 1111 1111 1111 0000 0000 hash0000 0000 0000 0000 0000 0000 0000 1111 n-1 15 &amp;运算----------------------------------------------------------0000 0000 0000 0000 0000 0000 0000 0000 索引位置为0 在存储一个key,假设key的hashcode高位变化较大1000 1000 0001 0000 1111 1111 0000 0000 hash值0000 0000 0000 0000 0000 0000 0000 1111 n-1 15 &amp;运算-------------------------------------------------------0000 0000 0000 0000 0000 0000 0000 0000 索引位置为0 如果当n的值,即数组长度很小时，假设是默认的16的话,这样的值和hashCode()直接做按位与操作，实际上只使用了哈希值的后4位。如果当哈希值的高位变化很大，低位变化很小,这样就很容易造成哈希冲突了，所以这里把高低位都利用起来,从而解决了这个问题。也是一种为了降低hash冲突的优化手段 ​ 为什么这里需要将高位数据移位到低位进行异或运算呢？这是因为有些数据计算出的哈希值差异主要在高位，而 HashMap 里的哈希寻址是忽略容量以上的高位的，那么这种处理就可以有效避免类似情况下的哈希碰撞。 2.2 hashmap的put()方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/** * Associates the specified value with the specified key in this map. * If the map previously contained a mapping for the key, the old * value is replaced. * * @param key key with which the specified value is to be associated * @param value value to be associated with the specified key * @return the previous value associated with &lt;tt&gt;key&lt;/tt&gt;, or * &lt;tt&gt;null&lt;/tt&gt; if there was no mapping for &lt;tt&gt;key&lt;/tt&gt;. * (A &lt;tt&gt;null&lt;/tt&gt; return can also indicate that the map * previously associated &lt;tt&gt;null&lt;/tt&gt; with &lt;tt&gt;key&lt;/tt&gt;.) */public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;/** * Implements Map.put and related methods. * * @param hash hash for key * @param key the key * @param value the value to put * @param onlyIfAbsent if true, don&#x27;t change existing value 如果为true表示不更改现有的值 * @param evict if false, the table is in creation mode. 表示table为创建状态 * @return previous value, or null if none */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; // p表示当前的节点。n表示表的长度 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // hashmap的table表的初始化操作,是在这里进行的。第一次执行的时候,会先在这里进行初始化操作 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 通过上述hash()方法得到hash值和数组长度进行按位与运算，得到元素的存储位置，如果table表的位置为空，就直接进行存储操作 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; // 产生了hash碰撞,走下述代码 Node&lt;K,V&gt; e; K k; // 这里的p的值是上面p = tab[i = (n - 1) &amp; hash ,if语句体虽然没有执行，但是这一段代码是否执行的,判断hash值和key值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 判断当前table中的p节点是不是树节点 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 遍历链表的,取下一个位置存放新增的元素,这里采用的是尾插法，a.横竖都要遍历链表的长度是否大于树化的阈值，所以遍历的过程中，就直接插入元素了b.可能的因素就是遍历的过程中要比较key值是否相同，和jdk1.7有些不同 for (int binCount = 0; ; ++binCount) &#123; //如果下一个位置为空,就直接连接在链表后面 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 如果链表的长度大于8个时,就进行链表转红黑树的操作 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; // 如果链表的下一个元素不为空,比较hash值和key值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // key值存在,就替换value值。新插入的元素的value值,替换原来这个key的value值 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // hashmap的键值对个数大于扩容的阈值,就进行扩容操作 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 如图示：桶长度（bucket) 为8的数组 2.5 TreeifyBin()123456789101112131415161718192021222324252627282930313233343536373839/** * Replaces all linked nodes in bin at index for given hash unless * table is too small, in which case resizes instead. 替换指定哈希表的索引桶中所有的连接节点，除非表太小，否则将修改大小 */final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; /* 如果元素数组为空 或者 数组长度小于 树结构化的最小阈值（MIN_TREEIFY_CAPACITY=64） ,就进行扩容操作.对于这个值可以理解为：如果元素数组长度小于这个值，没有必要去进行结构转换.目的是 如果数组很小,那么转红黑树,遍历效率要低一些，这时进行扩容操作，重新计算哈希值，链表的长度有可能就变短了。数据会放入到数组中，这样相对来说效率会高一些 */ if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); //如果元素数组长度已经大于等于了 MIN_TREEIFY_CAPACITY，那么就有必要进行结构转换了 // 根据hash值和数组长度进行取模运算后，得到链表的首节点 else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; //定义几个变量，hd代表头节点，tl代表尾节点 TreeNode&lt;K,V&gt; hd = null, tl = null; do &#123; //先把e节点转成TreeNode类型，并赋值给p TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); //如果尾节点tl为空，则说明还没有根节点，试想下，这时元素数目都超过8个了，还能没有尾节点么，所以没有尾节点只能说明还没设置根节点 if (tl == null) //设置根节点，把p赋值给根节点hd hd = p; else &#123; //把tl设置为p的前一节点 p.prev = tl; //把p设置为tl的后继节点，这两步其实就是我指向你，你指向我的关系，为了形成双向链表 tl.next = p; &#125; //把首节点设置成p后，把p赋值给尾节点tl，然后会再取链表的下一个节点，转成TreeNode类型后再赋值给p，如此循环 tl = p; //取下一个节点，直到下一个节点为空，也就代表这链表遍历好了 &#125; while ((e = e.next) != null); //用新生成的双向链表替代旧的单向链表，其实就是把这个数组对应的位置重新赋值成新双向链表的首节点 if ((tab[index] = hd) != null) hd.treeify(tab); &#125;&#125; bin 的数量大于 TREEIFY_THRESHOLD 时：如果容量小于 MIN_TREEIFY_CAPACITY，只会进行简单的扩容。如果容量大于 MIN_TREEIFY_CAPACITY ，则会进行树化改造。 本质上这是个安全问题。因为在元素放置过程中，如果一个对象哈希冲突，都被放置到同一个桶里，则会形成一个链表，我们知道链表查询是线性的，会严重影响存取的性能。 最后一行才是转红黑树的操作 2.6 Treeify()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 /** * Forms tree of the nodes linked from this node. */final void treeify(Node&lt;K,V&gt;[] tab) &#123; // 定义树的根节点 TreeNode&lt;K,V&gt; root = null; // 遍历链表，x指向当前节点、next指向下一个节点 for (TreeNode&lt;K,V&gt; x = this, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; // 如果根节点为空，则把当前节点当做根节点。根据红黑树性质，根节点一定为黑色。 if (root == null) &#123; x.parent = null; x.red = false; root = x; &#125; else &#123; // 根节点已经存在的情况 // 取得当前节点和当前节点的hash值 K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; // 从根节点遍历。这一步主要就是为了，判断当前节点应该在树的左侧还是右侧 for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; // dir代表方向 ph表示树节点的hash值。即TreeNodehash值 K pk = p.key; if ((ph = p.hash) &gt; h) // 如果当前树节点hash值 大于 当前链表节点的hash值 dir = -1; // 标识当前链表节点会放到当前树节点的左侧 else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); // 保存当前树节点 TreeNode&lt;K,V&gt; xp = p; // 判断如果dir&lt;=0 说明当前节点的hash值小于当前树节点的hash值。需要把当前节点放置在当前树节点的左侧 // 判断如果dir&gt;0 说明当前节点的hash值大于当前树节点的hash值。 需要把当前节点放置在当前树节点的右侧 // p的左右节点存在不为空的情况,p节点就是当前遍历的树节点，说明该节点还有子节点。继续循环查找当前节点x的应该在哪里爸爸节点下面插入元素 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; // 元素插入之后，需要进行红黑树的自平衡操作,重新确定根节点的值 root = balanceInsertion(root, x); break; &#125; &#125; &#125; &#125; moveRootToFront(tab, root);&#125; 2.7 红黑树自平衡操作2.71balanceInsertion()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172static &lt;K,V&gt; TreeNode&lt;K,V&gt; balanceInsertion(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; x) &#123; // 插入节点是黑色 x.red = true; // xp为父节点 xpp：为爷爷节点 xxpl是爷爷的左节点 xppr：是爷爷节点的右节点 for (TreeNode&lt;K,V&gt; xp, xpp, xppl, xppr;;) &#123; // 当前节点的父节点不存在,当前节点就是根节点 if ((xp = x.parent) == null) &#123; x.red = false; return x; &#125; // 父节点是黑色。当前节点可以直接插入,所以父节点就是根节点。 父节点如果是黑色，则爷爷节点一定不存在 else if (!xp.red || (xpp = xp.parent) == null) return root; // 如果父节点是爷爷节点的左节点 if (xp == (xppl = xpp.left)) &#123; // 如果爷爷节点的右节点不为空 同时是红节点 // a.将爸爸和叔叔节点变成黑色 b.将爷爷节点变成红色 c.并且将爷爷节点当成当前节点,进行下一轮的处理 if ((xppr = xpp.right) != null &amp;&amp; xppr.red) &#123; xppr.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; else &#123; // 如果父节点是爷爷节点的左节点 插入节点时爷爷节点的右节点 if (x == xp.right) &#123; // a.已爸爸为节点进行一次左旋操作,得到(LL双红的情况 4.2.1) 然后指定爸爸节点为当前节点,执行下一轮的操作 root = rotateLeft(root, x = xp); xpp = (xp = x.parent) == null ? null : xp.parent; &#125; // a.将爸爸节点染成黑色 b.将爷爷染成红色 c.然后已爷爷节点进行右旋操作 if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateRight(root, xpp); &#125; &#125; &#125; &#125; else &#123; // 叔叔节点不为空,同事叔叔节点为红色 // a.将爸爸和叔叔节点变成黑色 b.将爷爷节点变成红色 c.并且将爷爷节点当成当前节点,进行下一轮的处理 if (xppl != null &amp;&amp; xppl.red) &#123; xppl.red = false; xp.red = false; xpp.red = true; x = xpp; &#125; else &#123; // 插入节点为其父节点的左子节点(RL情况) a.以爸爸节点进行一次右旋,得到RR双红的场景( RR情况 4.3.1).然后指定爸爸节点为当前节点,进行下一轮的操作 if (x == xp.left) &#123; root = rotateRight(root, x = xp); // 平衡过后,重新定义爷爷节点的变量值 xpp = (xp = x.parent) == null ? null : xp.parent; &#125; // 插入节点为其父节点的右子节点(RR情况) a.将爸爸染成黑色节点 b.将爷爷染成红色 c.然后已爷爷节点进行左旋操作 if (xp != null) &#123; xp.red = false; if (xpp != null) &#123; xpp.red = true; root = rotateLeft(root, xpp); &#125; &#125; &#125; &#125; &#125;&#125; 2.72 红黑树左旋rotateLeft12345678910111213141516171819// Red-black tree methods, all adapted from CLRstatic &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateLeft(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; TreeNode&lt;K,V&gt; r, pp, rl; if (p != null &amp;&amp; (r = p.right) != null) &#123; if ((rl = p.right = r.left) != null) rl.parent = p; if ((pp = r.parent = p.parent) == null) (root = r).red = false; else if (pp.left == p) pp.left = r; else pp.right = r; r.left = p; p.parent = r; &#125; return root;&#125; 2.73 红黑树右旋rotateRight123456789101112131415161718static &lt;K,V&gt; TreeNode&lt;K,V&gt; rotateRight(TreeNode&lt;K,V&gt; root, TreeNode&lt;K,V&gt; p) &#123; TreeNode&lt;K,V&gt; l, pp, lr; if (p != null &amp;&amp; (l = p.left) != null) &#123; if ((lr = p.left = l.right) != null) lr.parent = p; if ((pp = l.parent = p.parent) == null) (root = l).red = false; else if (pp.right == p) pp.right = l; else pp.left = l; l.right = p; p.parent = l; &#125; return root;&#125; 转换后的形态大约是 2.8 扩容resize() 方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798/** * Initializes or doubles table size. If null, allocates in * accord with initial capacity target held in field threshold. * Otherwise, because we are using power-of-two expansion, the * elements from each bin must either stay at same index, or move * with a power of two offset in the new table. * 初始化或者扩容表的长度为2倍 * @return the table */ final Node&lt;K,V&gt;[] resize() &#123; // 旧表，第一次执行时，oldTabl为空 Node&lt;K,V&gt;[] oldTab = table; // 表的容量 int oldCap = (oldTab == null) ? 0 : oldTab.length; // 要调整大小的下一个大小值，默认是0 int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 如果扩容前旧表的容量大于阈值，就设置为Integer的最大值 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 如果旧表的长度左移一位还小于表的最大容量，就扩容表的长度为旧表的二倍，域值也为原来的一倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults // 初始化新容器的大小，必须是2的幂，默认是16。表刚创建的时候，会执行到这里 newCap = DEFAULT_INITIAL_CAPACITY; // 默认的阈值是12 负载因子0.75* 默认的初始化容量16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; // 更新table表扩容时的的阈值 threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) // 假设一个容量为16的节点。newCap的值为16 下一次扩容时，容量为32扩大一倍 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; // 移动旧表的数据到新表中,移动的扩容中,需要重新的进行hash操作 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // 链表节点的处理 head是头节点 tail是尾部节点 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; // 获取当前节点的下一个节点，每一次循环e值会更新 next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; // 第一次slot的节点是头节点也是尾结点 if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; // 下一个节点设置为空 hiTail.next = null; // slot节点的数+旧表的容量 存储新值 newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; // 返回扩容后新表的长度 return newTab; &#125; 依据 resize 源码，不考虑极端情况（容量理论最大极限由 MAXIMUM_CAPACITY 指定，数值为 1&lt;&lt;30，也就是 2 的 30 次方），我们可以归纳为：阈值等于（负载因子）x（容量），如果构建 HashMap 的时候没有指定它们，那么就是依据相应的默认常量值。阈值通常是以倍数进行调整 （newThr = oldThr &lt;&lt; 1），我前面提到，根据 putVal 中的逻辑，当元素个数超过阈值大小时，则调整 Map 大小。扩容后，需要将老的数组中的元素重新放置到新的数组，这是扩容的一个主要开销来源。 2.41 hashmap扩容的条件1.当HashMap中的元素个数超过 数组长度 * loadFactor(负载因子)时,就会进行数组扩容,扩容为原来的2倍 例如：数组长度默认是16，负载因子默认0.75 ，即当数组中 元素个数大于12的时候，就会进行扩容，数组长度变为32 2.在链表转换红黑树的时候，如果桶的数量不足64.也会进行扩容操作 2.42 扩容后的元素怎么存储进行扩容，会伴随着一次重新hash分配，并且会遍历hash表中所有的元素,是非常耗时的。在编写程序中,要尽量避免resize。HashMap在进行扩容时，使用的rehash方式非常巧妙，因为每次扩容都是翻倍，与原来计算的(n-1)&amp;hash的结果相比，只是多了一个bit位，所以节点要么就在原来的位置，要么就被分配到**”原位置+旧容量”**这个位置。 123456789101112131415161718192021222324&amp; (按位与运算) : 运算规则:相同的二进制数位上,都是1的时候，结果为1,否则为零。^ (按位异或运算) :运算规则:相同的二进制数位上，数字相同，结果为0，不同为1.代码中通过这个hash &amp; (n-1) 得到存储元素的位置0000 0000 0000 0000 0000 0000 0001 0000 n = 160000 0000 0000 0000 0000 0000 0000 1111 n-1 = 15假设hashcode生成的值为 如下2个0000 0000 0000 0000 1111 1111 0000 0111 hash(key1)0000 0000 0000 0000 1111 1111 0001 0111 hash(key2) 扩容后的hash值,比 扩容前多了个比特位计算开始0000 0000 0000 0000 0000 0000 0001 0000 n = 16 扩容前0000 0000 0000 0000 0000 0000 0000 1111 n-1 = 150000 0000 0000 0000 1111 1111 0000 0111 hash(key1)---------------------------------------------------------0000 0000 0000 0000 0000 0000 0000 0111 hash &amp; (n-1) = 7 索引位置为70000 0000 0000 0000 0000 0000 0010 0000 n=32 扩容一倍后0000 0000 0000 0000 0000 0000 0001 1111 n-1 = 310000 0000 0000 0000 1111 1111 0001 0111 hash(key2)------------------------------------------------------0000 0000 0000 0000 0000 0000 0001 0111 hash &amp; (n-1) = 23 索引位置为7+16(旧的数组容量) 结论：计算新的索引，高位是0那么存储到原来索引位置，如果高位是1那么存储到原来索引+旧的数组长度位置。因此，我们在扩充HashMap的时候，不需要重新计算hash,只需要看看原来的hash值新增的那个bit是1还是0就可以了，是0的话索引没变,是1的话索引变成原索引+oldCap(原位置+旧容量)。正是因为这样巧妙的rehash方式，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1是是随机的，在resize的过程中保证 了rehash之后每个桶上的节点数一定小于等于原来桶上的节点数,保证了rehash之后不会出现更严重的hash冲突,均匀的把之前的冲突的节点分散到新的桶中了。 2.4.3 hashmap容量初始化如果我们确切的知道我们有多少键值对需要存储，那么我们在初始化HashMap的时候就应该指定它的容量，防止HashMap自动扩容，影响使用效率。 2.4.4 多线程下hashmap造成cpu100%的问题hashmap不同于hashtable.他是线程不安全的，当多线程并发扩容时就可能会出现环形引用的问题，从而导致死循环的出现，一直死循环就会导致 CPU 运行 100%，但是Oracle认为这不是错误,是因为Hashmap本身就是线程不安全的。Oracle官网的连接如下 1https:&#x2F;&#x2F;bugs.java.com&#x2F;bugdatabase&#x2F;view_bug.do?bug_id&#x3D;6423457 并提供给了我们，可以替换的为jdk1.5之后的 ConcurrentHashMap 或者使用性能较差的Hashtable 或 synchronizedMap 包装器 java并发包的作者原文评论如下： 12345678910111213Doug Lea writes:&quot;This is a classic symptom of an incorrectly synchronized use ofHashMap. Clearly, the submitters need to use a thread-safeHashMap. If they upgraded to Java 5, they could just useConcurrentHashMap. If they can&#39;t do this yet, they can useeither the pre-JSR166 version, or better, the unofficial backportas mentioned by Martin. If they can&#39;t do any of these, they canuse Hashtable or synchhronizedMap wrappers, and live with poorerperformance. In any case, it&#39;s not a JDK or JVM bug.&quot;I agree that the presence of a corrupted data structure alonedoes not indicate a bug in the JDK. 为什么并发的时候，会导出环状呢，简单分析下 这个cpu100%的问题，实际工作中上我也没有遇到过，参考了下网上的文章： 原文链接：https://blog.csdn.net/hao134838/article/details/107220317/ JDK1.7的源码： 123456789101112131415void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; // 线程一执行此处 if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 假设 HashMap 的默认大小为 2，HashMap 本身中有一个键值 key(5)，我们再使用两个线程：t1 添加 key(3)，t2 添加 key(7)，首先两个线程先把 key(3) 和 key(7) 都添加到 HashMap 中，此时因为 HashMap 的长度不够用了就会进行扩容操作，然后这时线程 t1 在执行到 Entry&lt;K,V&gt; next = e.next 时，此时线程t1中的e节点指向了key(3),然后next指针指向了key(7)。然后让出了cpu的执行权。对于线程t2来说，添加key(7) 的时候，会进行扩容操作，扩容的时候，会再次进行rehash操作，重新计算位置，然后把链表的位置更改为了e节点为key7，他的next指向key3.此时t1线程又重新获取到了cpu的执行权。对于t1线程来说，key(3)指向的是key(7)，由于线程t2把key(7)的下一个指向变成了key(3)，对于t1来说，key(7) 的next就是key(3) .这样就造成key(3)和key(7)的循环引用问题，而死循环就是导致cpu100%的原因 2.4.5 hashmap的size不准确造成的诡异问题略 3.HashTable、HashMap、TreeMap不同Hashtable 是早期 Java 类库提供的一个哈希表实现，本身是同步的，不支持 null 键和值，由于同步导致的性能开销，所以已经很少被推荐使用。 HashMap 是应用更加广泛的哈希表实现，行为上大致上与 HashTable 一致，主要区别在于 HashMap 不是同步的，支持 null 键和值等。通常情况下，HashMap 进行 put 或者 get 操作，可以达到常数时间的性能，所以它是绝大部分利用键值对存取场景的首选 TreeMap 则是基于红黑树的一种提供顺序访问的 Map，和 HashMap 不同，它的 get、put、remove 之类操作都是 O（log(n)）的时间复杂度，具体顺序可以由指定的 Comparator 来决定，或者根据键的自然顺序来判断。 类图如下： HashMap 的性能表现非常依赖于哈希码的有效性，请务必掌握 hashCode 和 equals 的一些基本约定 比如： equals 相等，hashCode 一定要相等。 重写了 hashCode 也要重写 equals。 hashCode 需要保持一致性，状态改变返回的哈希值仍然要一致。 LinkedHashMap 通常提供的是遍历顺序符合插入顺序，它的实现是通过键值对维护一个双向链表。 对于 TreeMap，它的整体顺序是由键的顺序关系决定的，通过 Comparator 或 Comparable（自然顺序）来决定。","categories":[],"tags":[{"name":"Java基础","slug":"Java基础","permalink":"https://lswisdom.github.io/tags/Java%E5%9F%BA%E7%A1%80/"}]},{"title":"证书生成方式","slug":"linux/证书生成方式","date":"2021-07-25T09:43:36.000Z","updated":"2021-08-15T14:34:39.087Z","comments":true,"path":"posts/221454774/","link":"","permalink":"https://lswisdom.github.io/posts/221454774/","excerpt":"","text":"为服务器生成证书1keytool -genkey -v -alias tomcat -keyalg RSA -keystore E:\\download\\tomcat.keystore -validity 36500 为客户端生成证书1keytool -genkey -v -alias mykey -keyalg RSA -storetype PKCS12 -keystore E:\\download\\mykey.p12 会生成一个客户端的证书 让服务器信任客户端证书1keytool -export -alias mykey -keystore E:\\downlaod\\mykey.p12 -storetype PKCS12 -storepass 123456 -rfc -file E:\\downlaod\\mykey.cer 将该文件导入服务器的证书库，添加一个信任证书命令1keytool -import -v -file E:\\download\\mykey.cer -keystore E:\\download\\tomcat.keystore 查看证书命令：1keytool -list -keystore E:\\download\\tomcat.keystore 让客户端信任服务器证书1keytool -keystore E:\\download\\tomcat.keystore -export -alias tomcat -file E:\\download\\tomcat.cer(tomcat为你设置服务器端的证书名) 配置tomcat的服务器123456789101112131415&lt;Connector port=&quot;8443&quot; protocol=&quot;org.apache.coyote.http11.Http11NioProtocol&quot; SSLEnabled=&quot;true&quot; maxThreads=&quot;150&quot; scheme=&quot;https&quot; secure=&quot;true&quot; clientAuth=&quot;false&quot; sslProtocol=&quot;TLS&quot; keystoreFile=&quot;E:\\download\\tomcat.keystore&quot; keystorePass=&quot;123456&quot; /&gt; 把cas里面的证书位置替换一下cas/WEB-INF/classes/application.properties 1server.ssl.key-store&#x3D;E:\\download\\tomcat.keystore 在e:download文件夹中执行cmd 会生成一个证书 1keytool -import -alias tomcat -keystore cacerts -file tomcat.cer 复制该文件，并替换java目录下面的证书文件 1C:\\Program Files\\Java\\jdk1.8.0_251\\jre\\lib\\security","categories":[],"tags":[]},{"title":"Centos7安装Redis","slug":"linux/Centos7安装Redis","date":"2021-07-25T09:17:42.000Z","updated":"2021-08-15T14:33:42.889Z","comments":true,"path":"posts/2813726945/","link":"","permalink":"https://lswisdom.github.io/posts/2813726945/","excerpt":"","text":"一、CentOS7单节点安装Redis注意：安装redis需要先将官网下载的源码进行编译，编译依赖gcc环境，如果没有gcc环境，需要安装,否则编译时会报错 1yum install make cmake gcc gcc-c++ 1. 下载redis安装包1wget http:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-5.0.0.tar.gz 2.解压1tar -xzvf redis-5.0.0.tar.gz 3.进入redis目录下编译1make 4.指定安装目录安装，PREFIX参数指定redis的安装目录，命令如下 1make install PREFIX&#x3D;&#x2F;usr&#x2F;local&#x2F;software&#x2F;redis 5.启动redis进入安装后的路径，就是cd /usr/local/software/redis 在bin目录下执行 cd bin 1 .&#x2F;redis-server 这是redis的前台启动，缺点是ssh命令窗口关了之后服务就会停止 6.设置后台启动 把redis源码包解压目录下的redis.conf复制到安装目录的bin下 1cp &#x2F;usr&#x2F;local&#x2F;software&#x2F;redis-5.0.0&#x2F;redis.conf &#x2F;usr&#x2F;local&#x2F;software&#x2F;redis&#x2F;bin 修改配置文件（bin下面的redis.conf）：redis默认只能本机访问，要把下面两句改一下才能远程访问 #bind 127.0.0.1 注释掉 关闭保护模式 protected-mode no 以守护进程后台模式运行 daemonize yes 在bin目录下执行./redis-server redis.conf 检查是否启动完成 1ps ef | grep redis 7.开放端口号有时候可能会出现连接不上的情况，要注意开放端口号 打开6379端口防火墙方法： 执行命令打开6379端口防火墙，看到success字样就表明添加成功 1firewall-cmd --zone&#x3D;public --add-port&#x3D;6379&#x2F;tcp --permanent 然后重新启动防火墙，看到success字样就表明重启成功 1firewall-cmd --reload 验证是否生效 1firewall-cmd --zone&#x3D;public --query-port&#x3D;6379&#x2F;tcp 开放完端口号后，连接一下就可以了 8.redis.conf配置详解redis.conf 里面有一些配置，可以根据需要修改，比如说添加认证密码。修改端口号等 redis.conf配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331. Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程daemonize no2. 当Redis以守护进程方式运行时，Redis默认会把pid写入&#x2F;var&#x2F;run&#x2F;redis.pid文件，可以通过pidfile指定pidfile &#x2F;var&#x2F;run&#x2F;redis.pid3. 指定Redis监听端口，默认端口为6379，为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字port 63794. 绑定的主机地址bind 127.0.0.15.当 客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能timeout 3006. 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verboseloglevel verbose7. 日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给&#x2F;dev&#x2F;nulllogfile stdout8. 设置数据库的数量，默认数据库为0，可以使用SELECT &lt;dbid&gt;命令在连接上指定数据库iddatabases 169. 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合save &lt;seconds&gt; &lt;changes&gt;Redis默认配置文件中提供了三个条件：save 900 1save 300 10save 60 10000分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。10. 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大rdbcompression yes11. 指定本地数据库文件名，默认值为dump.rdbdbfilename dump.rdb12. 指定本地数据库存放目录dir .&#x2F;13. 设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步slaveof &lt;masterip&gt; &lt;masterport&gt;14. 当master服务设置了密码保护时，slav服务连接master的密码masterauth &lt;master-password&gt;15. 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH &lt;password&gt;命令提供密码，默认关闭requirepass foobared16. 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息maxclients 12817. 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区maxmemory &lt;bytes&gt;18. 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为noappendonly no19. 指定更新日志文件名，默认为appendonly.aofappendfilename appendonly.aof20. 指定更新日志条件，共有3个可选值： no：表示等操作系统进行数据缓存同步到磁盘（快） always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） everysec：表示每秒同步一次（折中，默认值）appendfsync everysec21. 指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析Redis的VM机制）vm-enabled no22. 虚拟内存文件路径，默认值为&#x2F;tmp&#x2F;redis.swap，不可多个Redis实例共享vm-swap-file &#x2F;tmp&#x2F;redis.swap23. 将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0vm-max-memory 024. Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值vm-page-size 3225. 设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，，在磁盘上每8个pages将消耗1byte的内存。vm-pages 13421772826. 设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4vm-max-threads 427. 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启glueoutputbuf yes28. 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法hash-max-zipmap-entries 64hash-max-zipmap-value 51229. 指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍）activerehashing yes30. 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件include &#x2F;path&#x2F;to&#x2F;local.confredis.conf配置","categories":[{"name":"Redis","slug":"Redis","permalink":"https://lswisdom.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://lswisdom.github.io/tags/Redis/"}]},{"title":"Centos7开启防火墙命令","slug":"linux/Centos7开启防火墙命令","date":"2021-07-25T09:13:26.000Z","updated":"2021-08-15T14:33:50.627Z","comments":true,"path":"posts/3592562801/","link":"","permalink":"https://lswisdom.github.io/posts/3592562801/","excerpt":"","text":"打开6379端口防火墙方法： 执行命令打开6379端口防火墙，看到success字样就表明添加成功 1firewall-cmd --zone&#x3D;public --add-port&#x3D;6379&#x2F;tcp --permanent 然后重新启动防火墙，看到success字样就表明重启成功 1firewall-cmd --reload 最后可以输入命令验证6379端口打开是否生效，看到yes及表示生效 1firewall-cmd --zone&#x3D;public --query-port&#x3D;6379&#x2F;tcp 重试一下，就可以连接了","categories":[{"name":"linux","slug":"linux","permalink":"https://lswisdom.github.io/categories/linux/"}],"tags":[]},{"title":"centos7安装JDK","slug":"linux/centos7安装JDK","date":"2021-07-25T08:54:55.000Z","updated":"2021-08-15T14:33:44.179Z","comments":true,"path":"posts/3393234349/","link":"","permalink":"https://lswisdom.github.io/posts/3393234349/","excerpt":"","text":"基本环境配置1.CentOS7安装JDK1.检查系统中是否存在JDKCentOS一般会自带两个jdk，使用命令查看 12345678910rpm -qa | grep javarpm -e --nodeps 要卸载的包 (包通过上面的指令可以获取到)]如：rpm -e --nodeps java-1.7.0-openjdk-1.7.0.99-2.6.5.1.el6.x86_64rpm -e --nodeps java-1.6.0-openjdk-1.6.0.38-1.13.10.4.el6.x86_64rpm:用于管理套件-e:表示删除指定的套件-q:使用询问模式，当遇到任何问题时，rpm指令会先询问用户。-a:查询所有套件。|:把前一个命令原本要bai输出到屏幕的数据du当作是后一个命令的标准输入 2.上传JDK包上传JDK包到/usr/local/software目录下面 执行解压JDK安装文件 1tar -zxvf jdk-8u151-linux-x64.tar.gz 3.配置环境变量vim /etc/profile 在文件最末尾加上下面几句话 12345export JAVA_HOME=/usr/local/software/jdk1.8.0_151export JRE_HOME=/usr/local/software/jdk1.8.0_151/jreexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/libexport PATH=$PATH:$JAVA_HOME/bin 保存退出 让配置生效： source /etc/profile 检查环境： 运行java –version能看到版本信息","categories":[{"name":"linux","slug":"linux","permalink":"https://lswisdom.github.io/categories/linux/"}],"tags":[{"name":"Linux环境安装","slug":"Linux环境安装","permalink":"https://lswisdom.github.io/tags/Linux%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"}]},{"title":"centos7下安装ElasticSearch","slug":"linux/centos7下安装ElasticSearch","date":"2021-04-18T11:25:45.000Z","updated":"2022-10-29T07:18:45.968Z","comments":true,"path":"posts/3121942040/","link":"","permalink":"https://lswisdom.github.io/posts/3121942040/","excerpt":"","text":"CentOS7安装ElasticSearch7.121.安装JDK安装JDK 并配置环境变量,配置如下： 1234export JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;software&#x2F;jdk1.8.0_281export JRE_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;software&#x2F;jdk1.8.0_281&#x2F;jreexport CLASSPATH&#x3D;.:$JAVA_HOME&#x2F;lib&#x2F;dt.jar:$JAVA_HOME&#x2F;lib&#x2F;tools.jar:$JRE_HOME&#x2F;libexport PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin 2.安装ElasticSearch注意：我下载的是7版本,需要jdk11的支持，但是开发环境我选择 jdk1.8 ​下载6版本的可以使用jdk1.8 ElasticSearch文档地址：https://www.elastic.co/guide/en/elasticsearch/reference/current/elasticsearch-intro.htmlElasticSearch下载地址：https://www.elastic.co/cn/downloads/elasticsearch 1tar -zxvf elasticsearch-7.12.0-linux-x86_64.tar.gz 下载完成后的地址路径如下： 3.启动ES,并解决ES启动报错进入bin目录下面执行 ./elasticsearch 错误一：ES7.12 需要使用JDK11的版本报错，提示信息如下： 1234distribution with a bundled JDK, ensure the JAVA_HOME environment variable is not set.warning: usage of JAVA_HOME is deprecated, use ES_JAVA_HOMEFuture versions of Elasticsearch will require Java 11; your Java version from [/usr/local/software/jdk1.8.0_281/jre] does not meet this requirement. Consider switching to a distribution of Elasticsearch with a bundled JDK. If you are already using a distribution with a bundled JDK, ensure the JAVA_HOME environment variable is not set.[2021-04-18T19:54:57,709][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [MiWiFi-R3-srv] uncaught exception in thread [main] 这个意思提示我们，使用ES7.12需要jdk11的版本下载安装jdk11,并修改es需要的jdk环境变量修改vim elasticsearch 文件,并在文件最上方添加如下配置： 12345678910#配置自己的jdk11export JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;jdk-11.0.10&#x2F;export PATH&#x3D;$JAVA_HOME&#x2F;bin:$PATH#添加jdk判断if [ -x &quot;$JAVA_HOME&#x2F;bin&#x2F;java&quot; ]; then JAVA&#x3D;&quot;&#x2F;usr&#x2F;local&#x2F;jdk-11.0.10&#x2F;bin&#x2F;java&quot;else JAVA&#x3D;&#96;which java&#96;fi 再次启动ES ./elasticsearch 错误二：Es提示我们不可以使用root用户 提示信息如下： 123456789101112java.lang.RuntimeException: can not run elasticsearch as root at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:101) at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:168) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:397) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:75) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:116) at org.elasticsearch.cli.Command.main(Command.java:79) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:81)For complete error details, refer to the log at &#x2F;usr&#x2F;local&#x2F;software&#x2F;elasticsearch-7.12.0&#x2F;logs&#x2F;elasticsearch.log 解决办法：创建新用户，并用新用户进行启动创建用户组：groupadd esgroup创建es用户：useradd esuser -g esgroup -p 123456然后使用chown命令来让这个文件夹属于这个用户chown -R esuser:esgroup elasticsearch-7.12.0切换用户，使用新用户esuser进行服务启动 再次启动,启动成功后，使用curl 127.0.0.1:9200 进行访问，如果返回一段json。说明启动成功了 12345678910111213141516171819[root@MiWiFi-R3-srv bin]# curl 127.0.0.1:9200&#123; &quot;name&quot; : &quot;MiWiFi-R3-srv&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;htoKhKvMRfCpzscUQLWWAQ&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;7.12.0&quot;, &quot;build_flavor&quot; : &quot;default&quot;, &quot;build_type&quot; : &quot;tar&quot;, &quot;build_hash&quot; : &quot;78722783c38caa25a70982b5b042074cde5d3b3a&quot;, &quot;build_date&quot; : &quot;2021-03-18T06:17:15.410153305Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;8.8.0&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;6.8.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;6.0.0-beta1&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 但是ES默认支持localhost启动，修改配置文件config下面的vim elasticsearch.yml 如下图所示启动完成后，重新启动，又出现了新的警告信息 错误三：Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release这个是警告信息，垃圾回收器的问题，可以不修改 1Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. 错误四：提示：在启动Elasticsearch之前，您必须解决以下行中描述的要点提示信息如下： 123456ERROR: [3] bootstrap checks failed. You must address the points described in the following [3] lines before starting Elasticsearch.bootstrap check failure [1] of [3]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]bootstrap check failure [2] of [3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]bootstrap check failure [3] of [3]: the default discovery settings are unsuitable for production use; at least one of [discovery.seed_hosts, discovery.seed_providers, cluster.initial_master_nodes] must be configuredERROR: Elasticsearch did not exit normally - check the logs at &#x2F;usr&#x2F;local&#x2F;software&#x2F;elasticsearch-7.12.0&#x2F;logs&#x2F;elasticsearch.log 4.第一个错误，意思大概是：elasticsearch进程的文件描述符[4096]过低，增加到至少[65535]解决方案： 修改 vim /etc/security/limits.conf 在文件最下方加入如下配置：注意：第一列是你刚才创建的用户名称,我刚才创建的用户是esuser esuser soft nofile 65536esuser hard nofile 65536esuser soft nproc 4096esuser hard nproc 4096 5.第二个错误，意思大概是：虚拟机最大的虚拟内存区域。max_map_count[65530]太低，增加到至少[262144]解决办法：进入/etc 修改sysctl.conf文档，里面都是注释，添加如下面一行配置 1vm.max_map_count&#x3D;655360 推荐信息提示至少262144 保存信息wq注意：配置如果想要生效，需要执行如下配置： 6.第三个错误：解决办法：在elasticsearch的config目录下，修改elasticsearch.yml配置文件，将下面的配置加入到该配置文件中：注意：这里的node-1为node-name配置的值 1cluster.initial_master_nodes: [&quot;node-1&quot;] 注意默认情况下：node-name的值是注释掉的。这里要放开 7. 重新启动ES 看看效果,启动完成后,还是提示这个错误 check failure [1] of [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535]这种情况,可能是配置没有生效,重启虚拟机再看看效果重新启动完成后，再次启动提示start，表示已经正常启动了 8.windows电脑无法直接访问开放9200端口 12firewall-cmd --zone&#x3D;public --add-port&#x3D;9200&#x2F;tcp --permanentfirewall-cmd --reload 使用windows客户端访问，能看到正常返回json了","categories":[{"name":"linux","slug":"linux","permalink":"https://lswisdom.github.io/categories/linux/"}],"tags":[{"name":"Linux环境安装","slug":"Linux环境安装","permalink":"https://lswisdom.github.io/tags/Linux%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"}]},{"title":"使用CAS实现单点登录一","slug":"linux/CAS实现单点登录","date":"2021-04-14T23:32:10.000Z","updated":"2021-08-15T14:33:44.750Z","comments":true,"path":"posts/3633779439/","link":"","permalink":"https://lswisdom.github.io/posts/3633779439/","excerpt":"","text":"一、CAS服务器的搭建地址：github地址：https://github.com/apereo/cas-overlay-template 1.1 服务部署和测试clone下来依赖包,下载后的依赖包,需要手动创建src/main/resources 目录,并在该目录下面创建application.properties文件和log4j2.xml文件log4j2.xml文件,主要是为了设置CAS的日志输出目录application.properties目录：一些数据库配置、证书配置等配置完成后,打包部署在tomcat的webapp目录下面 等待CAS服务启动之后,通过http://localhost:8080/cas/login可以使用默认的账号：casuser Mellon登录 二、配置application.propertis 文件,通过数据库验证2.1 pom.xml中添加依赖文件123456789101112&lt;dependency&gt; &lt;groupId&gt;org.postgresql&lt;&#x2F;groupId&gt; &lt;artifactId&gt;postgresql&lt;&#x2F;artifactId&gt; &lt;version&gt;42.2.18.jre7&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt;&lt;!-- https:&#x2F;&#x2F;mvnrepository.com&#x2F;artifact&#x2F;org.apereo.cas&#x2F;cas-server-support-jdbc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apereo.cas&lt;&#x2F;groupId&gt; &lt;artifactId&gt;cas-server-support-jdbc&lt;&#x2F;artifactId&gt; &lt;version&gt;$&#123;cas.version&#125;&lt;&#x2F;version&gt;&lt;&#x2F;dependency&gt; 注释掉默认的用户配置： 1#cas.authn.accept.users&#x3D;casuser::Mellon 2.2 添加jdbc认证2.2.1 这是使用数据库连接的配置123456789cas.authn.jdbc.query[0].driverClass&#x3D;org.postgresql.Drivercas.authn.jdbc.query[0].url&#x3D;jdbc:postgresql:&#x2F;&#x2F;127.0.0.1:5432&#x2F;postgrescas.authn.jdbc.query[0].user&#x3D;postgrescas.authn.jdbc.query[0].password&#x3D;passwordcas.authn.jdbc.query[0].sql&#x3D;select * from sys_user where username&#x3D;?# 字段的列明cas.authn.jdbc.query[0].fieldPassword&#x3D;passwordcas.authn.jdbc.query[0].passwordEncoder.characterEncoding&#x3D;UTF-8 2.2.2 MD5加密配置如果密码进行加盐配置,需要增加如下配置,这样数据库中存储的事MD5加密的密码,CAS页面输入的密码还是原始的密码就可以登录 123cas.authn.jdbc.query[0].passwordEncoder.encodingAlgorithm&#x3D;MD5cas.authn.jdbc.query[0].passwordEncoder.type&#x3D;DEFAULTcas.authn.jdbc.query[0].passwordEncoder.characterEncoding&#x3D;UTF-8 2.2.3 密码加盐加密简单的MD5值太简单,需要加盐值的配置 12345678910111213141516# 加盐配置#配置数据库连接cas.authn.jdbc.encode[0].driverClass&#x3D;org.postgresql.Drivercas.authn.jdbc.encode[0].url&#x3D;jdbc:postgresql:&#x2F;&#x2F;127.0.0.1:5432&#x2F;postgrescas.authn.jdbc.encode[0].user&#x3D;postgrescas.authn.jdbc.encode[0].password&#x3D;password#加密迭代次数cas.authn.jdbc.encode[0].numberOfIterations&#x3D;1024# 数据库存放的动态盐值的字段列明cas.authn.jdbc.encode[0].saltFieldName&#x3D;PasswordSaltcas.authn.jdbc.encode[0].sql&#x3D;select * from sys_user where username&#x3D;?cas.authn.jdbc.encode[0].algorithmName&#x3D;MD5# 哪个字段作为密码字段cas.authn.jdbc.encode[0].passwordFieldName&#x3D;password application.properties文件的模板如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149### CAS Server Context Configuration#server.context-path&#x3D;&#x2F;casserver.port&#x3D;8443# CAS 的访问需要https证书,在这里配置server.ssl.key-store&#x3D;E:&#x2F;download&#x2F;tomcat.keystoreserver.ssl.key-store-password&#x3D;123456server.ssl.key-password&#x3D;123456server.max-http-header-size&#x3D;2097152server.use-forward-headers&#x3D;trueserver.connection-timeout&#x3D;20000server.error.include-stacktrace&#x3D;ALWAYSserver.compression.enabled&#x3D;trueserver.compression.mime-types&#x3D;application&#x2F;javascript,application&#x2F;json,application&#x2F;xml,text&#x2F;html,text&#x2F;xml,text&#x2F;plainserver.tomcat.max-http-post-size&#x3D;2097152server.tomcat.basedir&#x3D;build&#x2F;tomcatserver.tomcat.accesslog.enabled&#x3D;trueserver.tomcat.accesslog.pattern&#x3D;%t %a &quot;%r&quot; %s (%D ms)server.tomcat.accesslog.suffix&#x3D;.logserver.tomcat.min-spare-threads&#x3D;10server.tomcat.max-threads&#x3D;200server.tomcat.port-header&#x3D;X-Forwarded-Portserver.tomcat.protocol-header&#x3D;X-Forwarded-Protoserver.tomcat.protocol-header-https-value&#x3D;httpsserver.tomcat.remote-ip-header&#x3D;X-FORWARDED-FORserver.tomcat.uri-encoding&#x3D;UTF-8spring.http.encoding.charset&#x3D;UTF-8spring.http.encoding.enabled&#x3D;truespring.http.encoding.force&#x3D;true### CAS Cloud Bus Configuration#spring.cloud.bus.enabled&#x3D;false# Indicates that systemPropertiesOverride can be used.# Set to false to prevent users from changing the default accidentally. Default true.spring.cloud.config.allow-override&#x3D;true# External properties should override system properties.spring.cloud.config.override-system-properties&#x3D;false# When allowOverride is true, external properties should take lowest priority, and not override any# existing property sources (including local config files).spring.cloud.config.override-none&#x3D;false# spring.cloud.bus.refresh.enabled&#x3D;true# spring.cloud.bus.env.enabled&#x3D;true# spring.cloud.bus.destination&#x3D;CasCloudBus# spring.cloud.bus.ack.enabled&#x3D;trueendpoints.enabled&#x3D;falseendpoints.sensitive&#x3D;trueendpoints.restart.enabled&#x3D;falseendpoints.shutdown.enabled&#x3D;false# Control the security of the management&#x2F;actuator endpoints# The &#39;enabled&#39; flag below here controls the rendering of details for the health endpoint amongst other things.management.security.enabled&#x3D;truemanagement.security.roles&#x3D;ACTUATOR,ADMINmanagement.security.sessions&#x3D;if_requiredmanagement.context-path&#x3D;&#x2F;statusmanagement.add-application-context-header&#x3D;false# Define a CAS-specific &quot;WARN&quot; status code and its ordermanagement.health.status.order&#x3D;WARN, DOWN, OUT_OF_SERVICE, UNKNOWN, UP# Control the security of the management&#x2F;actuator endpoints# With basic authentication, assuming Spring Security and&#x2F;or relevant modules are on the classpath.security.basic.authorize-mode&#x3D;rolesecurity.basic.path&#x3D;&#x2F;cas&#x2F;status&#x2F;**# security.basic.enabled&#x3D;true# security.user.name&#x3D;casuser# security.user.password&#x3D;### CAS Web Application Session Configuration#server.session.timeout&#x3D;300server.session.cookie.http-only&#x3D;trueserver.session.tracking-modes&#x3D;COOKIE### CAS Thymeleaf View Configuration#spring.thymeleaf.encoding&#x3D;UTF-8spring.thymeleaf.cache&#x3D;truespring.thymeleaf.mode&#x3D;HTMLspring.thymeleaf.template-resolver-order&#x3D;100### CAS Log4j Configuration## logging.config&#x3D;file:&#x2F;etc&#x2F;cas&#x2F;log4j2.xmlserver.context-parameters.isLog4jAutoInitializationDisabled&#x3D;true### CAS AspectJ Configuration#spring.aop.auto&#x3D;truespring.aop.proxy-target-class&#x3D;true## 添加认证服务cas.tgc.secure&#x3D;falsecas.serviceRegistry.initFromJson&#x3D;true### CAS Authentication Credentials## 默认的登录正好和密码#cas.authn.accept.users&#x3D;casuser::Mellon# 加盐配置#配置数据库连接#cas.authn.jdbc.encode[0].driverClass&#x3D;org.postgresql.Driver#cas.authn.jdbc.encode[0].url&#x3D;jdbc:postgresql:&#x2F;&#x2F;127.0.0.1:5432&#x2F;postgres#cas.authn.jdbc.encode[0].user&#x3D;postgres#cas.authn.jdbc.encode[0].password&#x3D;password#加密迭代次数#cas.authn.jdbc.encode[0].numberOfIterations&#x3D;1024# 数据库存放的动态盐值的字段列明#cas.authn.jdbc.encode[0].saltFieldName&#x3D;PasswordSalt#cas.authn.jdbc.encode[0].sql&#x3D;select * from sys_user where username&#x3D;?#cas.authn.jdbc.encode[0].algorithmName&#x3D;MD5# 哪个字段作为密码字段#cas.authn.jdbc.encode[0].passwordFieldName&#x3D;passwordcas.authn.jdbc.query[0].driverClass&#x3D;org.postgresql.Drivercas.authn.jdbc.query[0].url&#x3D;jdbc:postgresql:&#x2F;&#x2F;127.0.0.1:5432&#x2F;postgrescas.authn.jdbc.query[0].user&#x3D;postgrescas.authn.jdbc.query[0].password&#x3D;password#cas.authn.jdbc.query[0].numberOfIterations&#x3D;1024#cas.authn.jdbc.query[0].saltFieldName&#x3D;PasswordSaltcas.authn.jdbc.query[0].sql&#x3D;select * from sys_user where username&#x3D;?cas.authn.jdbc.query[0].passwordEncoder.encodingAlgorithm&#x3D;MD5cas.authn.jdbc.query[0].fieldPassword&#x3D;passwordcas.authn.jdbc.query[0].passwordEncoder.type&#x3D;DEFAULTcas.authn.jdbc.query[0].passwordEncoder.characterEncoding&#x3D;UTF-8 log4j2.xml文件的模板如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot; ?&gt;&lt;!-- Specify the refresh internal in seconds. --&gt;&lt;Configuration monitorInterval&#x3D;&quot;5&quot; packages&#x3D;&quot;org.apereo.cas.logging&quot;&gt; &lt;Properties&gt; &lt;Property name&#x3D;&quot;baseDir&quot;&gt;D:\\device-logs&lt;&#x2F;Property&gt; &lt;&#x2F;Properties&gt; &lt;Appenders&gt; &lt;Console name&#x3D;&quot;console&quot; target&#x3D;&quot;SYSTEM_OUT&quot;&gt; &lt;PatternLayout pattern&#x3D;&quot;%d %p [%c] - &lt;%m&gt;%n&quot;&#x2F;&gt; &lt;&#x2F;Console&gt; &lt;RollingFile name&#x3D;&quot;file&quot; fileName&#x3D;&quot;$&#123;baseDir&#125;&#x2F;cas.log&quot; append&#x3D;&quot;true&quot; filePattern&#x3D;&quot;$&#123;baseDir&#125;&#x2F;cas-%d&#123;yyyy-MM-dd-HH&#125;-%i.log&quot;&gt; &lt;PatternLayout pattern&#x3D;&quot;%highlight&#123;%d %p [%c] - &lt;%m&gt;&#125;%n&quot;&#x2F;&gt; &lt;Policies&gt; &lt;OnStartupTriggeringPolicy &#x2F;&gt; &lt;SizeBasedTriggeringPolicy size&#x3D;&quot;10 MB&quot;&#x2F;&gt; &lt;TimeBasedTriggeringPolicy &#x2F;&gt; &lt;&#x2F;Policies&gt; &lt;DefaultRolloverStrategy max&#x3D;&quot;5&quot; compressionLevel&#x3D;&quot;9&quot;&gt; &lt;Delete basePath&#x3D;&quot;$&#123;baseDir&#125;&quot; maxDepth&#x3D;&quot;2&quot;&gt; &lt;IfFileName glob&#x3D;&quot;*&#x2F;*.log.gz&quot; &#x2F;&gt; &lt;IfLastModified age&#x3D;&quot;7d&quot; &#x2F;&gt; &lt;&#x2F;Delete&gt; &lt;&#x2F;DefaultRolloverStrategy&gt; &lt;&#x2F;RollingFile&gt; &lt;RollingFile name&#x3D;&quot;auditlogfile&quot; fileName&#x3D;&quot;$&#123;baseDir&#125;&#x2F;cas_audit.log&quot; append&#x3D;&quot;true&quot; filePattern&#x3D;&quot;$&#123;baseDir&#125;&#x2F;cas_audit-%d&#123;yyyy-MM-dd-HH&#125;-%i.log&quot;&gt; &lt;PatternLayout pattern&#x3D;&quot;%d %p [%c] - %m%n&quot;&#x2F;&gt; &lt;Policies&gt; &lt;OnStartupTriggeringPolicy &#x2F;&gt; &lt;SizeBasedTriggeringPolicy size&#x3D;&quot;10 MB&quot;&#x2F;&gt; &lt;TimeBasedTriggeringPolicy &#x2F;&gt; &lt;&#x2F;Policies&gt; &lt;DefaultRolloverStrategy max&#x3D;&quot;5&quot; compressionLevel&#x3D;&quot;9&quot;&gt; &lt;Delete basePath&#x3D;&quot;$&#123;baseDir&#125;&quot; maxDepth&#x3D;&quot;2&quot;&gt; &lt;IfFileName glob&#x3D;&quot;*&#x2F;*.log.gz&quot; &#x2F;&gt; &lt;IfLastModified age&#x3D;&quot;7d&quot; &#x2F;&gt; &lt;&#x2F;Delete&gt; &lt;&#x2F;DefaultRolloverStrategy&gt; &lt;&#x2F;RollingFile&gt; &lt;RollingFile name&#x3D;&quot;perfFileAppender&quot; fileName&#x3D;&quot;$&#123;baseDir&#125;&#x2F;perfStats.log&quot; append&#x3D;&quot;true&quot; filePattern&#x3D;&quot;$&#123;baseDir&#125;&#x2F;perfStats-%d&#123;yyyy-MM-dd-HH&#125;-%i.log&quot;&gt; &lt;PatternLayout pattern&#x3D;&quot;%m%n&quot;&#x2F;&gt; &lt;Policies&gt; &lt;OnStartupTriggeringPolicy &#x2F;&gt; &lt;SizeBasedTriggeringPolicy size&#x3D;&quot;10 MB&quot;&#x2F;&gt; &lt;TimeBasedTriggeringPolicy &#x2F;&gt; &lt;&#x2F;Policies&gt; &lt;DefaultRolloverStrategy max&#x3D;&quot;5&quot; compressionLevel&#x3D;&quot;9&quot;&gt; &lt;Delete basePath&#x3D;&quot;$&#123;baseDir&#125;&quot; maxDepth&#x3D;&quot;2&quot;&gt; &lt;IfFileName glob&#x3D;&quot;*&#x2F;*.log.gz&quot; &#x2F;&gt; &lt;IfLastModified age&#x3D;&quot;7d&quot; &#x2F;&gt; &lt;&#x2F;Delete&gt; &lt;&#x2F;DefaultRolloverStrategy&gt; &lt;&#x2F;RollingFile&gt; &lt;CasAppender name&#x3D;&quot;casAudit&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;auditlogfile&quot; &#x2F;&gt; &lt;&#x2F;CasAppender&gt; &lt;CasAppender name&#x3D;&quot;casFile&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;file&quot; &#x2F;&gt; &lt;&#x2F;CasAppender&gt; &lt;CasAppender name&#x3D;&quot;casConsole&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;console&quot; &#x2F;&gt; &lt;&#x2F;CasAppender&gt; &lt;CasAppender name&#x3D;&quot;casPerf&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;perfFileAppender&quot; &#x2F;&gt; &lt;&#x2F;CasAppender&gt; &lt;&#x2F;Appenders&gt; &lt;Loggers&gt; &lt;AsyncLogger name&#x3D;&quot;com.couchbase&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot; includeLocation&#x3D;&quot;true&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.apereo.cas.web.CasWebApplication&quot; level&#x3D;&quot;info&quot; additivity&#x3D;&quot;false&quot; includeLocation&#x3D;&quot;true&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.security&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.boot.autoconfigure.security&quot; level&#x3D;&quot;info&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.jasig.cas.client&quot; level&#x3D;&quot;info&quot; additivity&#x3D;&quot;false&quot; includeLocation&#x3D;&quot;true&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.apereo&quot; level&#x3D;&quot;info&quot; additivity&#x3D;&quot;false&quot; includeLocation&#x3D;&quot;true&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.apereo.services.persondir&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot; includeLocation&#x3D;&quot;true&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.apache&quot; level&#x3D;&quot;error&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.cloud&quot; level&#x3D;&quot;info&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.cloud.context&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.boot&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.aop&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.boot.actuate.autoconfigure&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.webflow&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.session&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.amqp&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.integration&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.messaging&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.web&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.orm.jpa&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.scheduling&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.thymeleaf&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.pac4j&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.opensaml&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;net.sf.ehcache&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;com.ryantenney.metrics&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;console&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;file&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;net.jradius&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.openid4java&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.ldaptive&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;com.hazelcast&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.context.annotation&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot; &#x2F;&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.boot.devtools&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot; &#x2F;&gt; &lt;AsyncLogger name&#x3D;&quot;org.jasig.spring&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.springframework.web.socket&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.apache.cxf&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.apache.http&quot; level&#x3D;&quot;off&quot; additivity&#x3D;&quot;false&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;perfStatsLogger&quot; level&#x3D;&quot;info&quot; additivity&#x3D;&quot;false&quot; includeLocation&#x3D;&quot;true&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casPerf&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.apereo.cas.web.flow&quot; level&#x3D;&quot;info&quot; additivity&#x3D;&quot;true&quot; includeLocation&#x3D;&quot;true&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncLogger name&#x3D;&quot;org.apereo.inspektr.audit.support&quot; level&#x3D;&quot;info&quot; includeLocation&#x3D;&quot;true&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casAudit&quot;&#x2F;&gt; &lt;AppenderRef ref&#x3D;&quot;casFile&quot;&#x2F;&gt; &lt;&#x2F;AsyncLogger&gt; &lt;AsyncRoot level&#x3D;&quot;error&quot;&gt; &lt;AppenderRef ref&#x3D;&quot;casConsole&quot;&#x2F;&gt; &lt;&#x2F;AsyncRoot&gt; &lt;&#x2F;Loggers&gt;&lt;&#x2F;Configuration&gt;","categories":[{"name":"linux","slug":"linux","permalink":"https://lswisdom.github.io/categories/linux/"}],"tags":[]},{"title":"CentOS7安装MYSLQ5.7","slug":"linux/CentOS7安装MySQL5.7","date":"2021-04-05T09:54:08.000Z","updated":"2021-07-25T09:09:47.326Z","comments":true,"path":"posts/1192933728/","link":"","permalink":"https://lswisdom.github.io/posts/1192933728/","excerpt":"","text":"CentOS7.3安装mysql第一步：下载和安装mysql源 1.先下载 mysql源安装包 1wget https:&#x2F;&#x2F;dev.mysql.com&#x2F;get&#x2F;mysql57-community-release-el7-11.noarch.rpm 2.安装mysql源 1yum -y localinstall mysql57-community-release-el7-11.noarch.rpm 第二步：在线安装Mysql 1yum -y install mysql-community-server 下载的东西比较多,要稍微等会 第三步：启动Mysql服务 1systemctl start mysqld 第四步：设置开机启动 123systemctl enable mysqldsystemctl daemon-reload 第五步：修改root本地登录密码 mysql安装完成之后，在/var/log/mysqld.log文件中给root生成了一个临时的默认密码。 [root@localhost ~]# vi /var/log/mysqld.log 这里的临时密码 eMV.R#mWe3ha [root@localhost ~]# mysql -u root -p Enter password: 输入临时密码 进入mysql命令行； 1234mysql&gt; ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;ZhipengWang2012@&#39;;Query OK, 0 rows affected (0.00 sec) 修改密码为 ZhipengWang2012@ (备注 mysql5.7默认密码策略要求密码必须是大小写字母数字特殊字母的组合，至少8位) 第七步：设置允许远程登录 Mysql默认不允许远程登录，我们需要设置下，并且防火墙开放3306端口； 123456mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;@LiShuai123&#39; WITH GRANT OPTION;Query OK, 0 rows affected, 1 warning (0.01 sec)mysql&gt; exit;Bye 退出下； centos7开启防火墙 123456789101112[root@localhost ~]# firewall-cmd --zone&#x3D;public --add-port&#x3D;3306&#x2F;tcp --permanentsuccess[root@localhost ~]# firewall-cmd --reloadsuccess[root@localhost ~]# 开放3306端口 第八步：配置默认编码为utf8 修改/etc/my.cnf配置文件，在[mysqld]下添加编码配置，如下所示： [mysqld] character_set_server=utf8 init_connect=’SET NAMES utf8’ [root@localhost ~]# vi /etc/my.cnf 编辑保存完 重启mysql服务； [root@localhost ~]# systemctl restart mysqld [root@localhost ~]# 查看下编码： 12345678910111213141516171819202122232425262728mysql&gt; show variables like &#39;%character%&#39;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | &#x2F;usr&#x2F;share&#x2F;mysql&#x2F;charsets&#x2F; |+--------------------------+----------------------------+8 rows in set (0.00 sec) 注:都安装成功了之后，那个tar.gz的压缩包就可以删掉了，节省磁盘空间 linux下的mysql默认是大小写敏感的，通过java操作数据库的时候会出现找不到表的情况，需要设置大小写不敏感 12vim &#x2F;etc&#x2F;my.cnf[mysqld]后添加添加lower_case_table_names&#x3D;1 12345虚拟机中mysql的地址如下：10.9.32.171账号root密码：root CentOs6.8安装mysql https://www.cnblogs.com/saneri/p/6617415.html","categories":[{"name":"linux","slug":"linux","permalink":"https://lswisdom.github.io/categories/linux/"}],"tags":[{"name":"环境安装","slug":"环境安装","permalink":"https://lswisdom.github.io/tags/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"}]}],"categories":[{"name":"k8s安装","slug":"k8s安装","permalink":"https://lswisdom.github.io/categories/k8s%E5%AE%89%E8%A3%85/"},{"name":"Redis","slug":"Redis","permalink":"https://lswisdom.github.io/categories/Redis/"},{"name":"linux","slug":"linux","permalink":"https://lswisdom.github.io/categories/linux/"}],"tags":[{"name":"运维环境部署","slug":"运维环境部署","permalink":"https://lswisdom.github.io/tags/%E8%BF%90%E7%BB%B4%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"},{"name":"前端框架","slug":"前端框架","permalink":"https://lswisdom.github.io/tags/%E5%89%8D%E7%AB%AF%E6%A1%86%E6%9E%B6/"},{"name":"Es学习","slug":"Es学习","permalink":"https://lswisdom.github.io/tags/Es%E5%AD%A6%E4%B9%A0/"},{"name":"Java基础","slug":"Java基础","permalink":"https://lswisdom.github.io/tags/Java%E5%9F%BA%E7%A1%80/"},{"name":"数据结构","slug":"数据结构","permalink":"https://lswisdom.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"Redis","slug":"Redis","permalink":"https://lswisdom.github.io/tags/Redis/"},{"name":"Linux环境安装","slug":"Linux环境安装","permalink":"https://lswisdom.github.io/tags/Linux%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"},{"name":"环境安装","slug":"环境安装","permalink":"https://lswisdom.github.io/tags/%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/"}]}